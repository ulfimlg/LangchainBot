{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY']=os.getenv(\"MY_OPENAI_API_KEY\")\n",
    "AZURE_DOC_API = os.getenv(\"AZURE_DOC_API\")\n",
    "AZURE_DOC_ENDPOINT = os.getenv(\"AZURE_DOC_ENDPOINT\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Ingestion\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "loader=TextLoader(\"speech.txt\")\n",
    "text_documents=loader.load()\n",
    "# text_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import AzureAIDocumentIntelligenceLoader\n",
    "\n",
    "file_path = os.getenv(\"TEST_FILE\")\n",
    "loader = AzureAIDocumentIntelligenceLoader(\n",
    "    api_endpoint=AZURE_DOC_ENDPOINT, api_key=AZURE_DOC_API, file_path=file_path, api_model=\"prebuilt-layout\"\n",
    ")\n",
    "\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FAISS Vector Database\n",
    "from langchain_community.vectorstores import FAISS\n",
    "db = FAISS.from_documents(documents, OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9th Grade Mathematics Test: Volume Calculation\n",
      "===\n",
      "\n",
      "Instructions: Answer all the questions. For multiple-choice questions, circle the correct answer. For fill-in-the-blank and show-your-work questions, write your answer clearly in the space provided.\n",
      "\n",
      "\n",
      "## Question 1 (Multiple Choice):\n",
      "\n",
      "What is the formula for calculating the volume of a cylinder?\n",
      "\n",
      "A) π r 2 h\n",
      "\n",
      "B) π r 2\n",
      "\n",
      "C) 2πrh\n",
      "\n",
      "D) 2πr(h + r)\n",
      "\n",
      "\n",
      "## Question 2 (Fill in the Blank):\n",
      "\n",
      "The volume of a cube is given by the formula \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_, where s represents the length of one side of the cube.\n",
      "\n",
      "\n",
      "## Question 3 (Show Your Work):\n",
      "\n",
      "Calculate the volume of a rectangular prism with a length of 10 cm, a width of 4 cm, and a height of 5 cm. Show your work.\n",
      "\n",
      "\n",
      "## Question 4 (Multiple Choice):\n",
      "\n",
      "A sphere has a radius of 3 meters. What is the volume of the sphere?\n",
      "\n",
      "A) 36π m 3\n",
      "\n",
      "B) 4π m 3\n",
      "\n",
      "C) 113.1 m 3 (Use 4/3πr 3 for calculation)\n",
      "\n",
      "D) 27π m 3\n",
      "\n",
      "\n",
      "## Question 5 (Text Answer):\n",
      "\n",
      "Explain in your own words why the formula for the volume of a pyramid is one-third the product of its base area and height.\n",
      "\n",
      "Answer Key\n",
      "===\n",
      "\n",
      "1\\. A) π r 2 h\n",
      "\n",
      "2\\. s 3 or \"s cubed\"\n",
      "\n",
      "3\\. Volume = Length × Width × Height = 10 cm × 4 cm × 5 cm = 200 cm³\n",
      "\n",
      "4\\. C) 113.1 m 3 (4/3 × π × 3^3 = 113.1 m 3 )\n",
      "\n",
      "5\\. The formula for the volume of a pyramid is one-third the product of its base area and height because it takes three pyramids of the same size to fill a prism that has the same base and height. This shows that a pyramid occupies one-third of the volume that a prism with the same base and height does.\n",
      "\n",
      "[0: \"Analysis:\\\\n\\\\nQuestion 1:\\\\n- Question Type and Content Description: Multiple choice question on limits.\\\\n- Correct Answer: B) 1\\\\n- Student's Answer: B) 1\\\\n- Cognitive Level: Knowledge\\\\n- Achievement Level: Achieved\\\\n- Potential Reasons: The student correctly identified the limit.\\\\n- Feedback and Strategies: Reinforce understanding of limit concepts.\\\\n\\\\nQuestion 2:\\\\n- Question Type and Content Description: Fill in the blank question on limits.\\\\n- Correct Answer: Asymptotic\\\\n- Student's Answer: Asymptotic\\\\n- Cognitive Level: Comprehension\\\\n- Achievement Level: Achieved\\\\n- Potential Reasons: The student remembered the definition of a limit.\\\\n- Feedback and Strategies: Encourage further exploration of limit concepts.\\\\n\\\\nQuestion 3:\\\\n- Question Type and Content Description: Worked out limit question.\\\\n- Correct Answer: limit of (3x - 2)/(2x + 1) as x approaches infinity = 3/2\\\\n- Student's Answer: limit of (3x - 2)/(2x + 1) as x approaches infinity = 3/2\\\\n- Cognitive Level: Application\\\\n- Achievement Level: Achieved\\\\n- Potential Reasons: The student correctly calculated the limit.\\\\n- Feedback and Strategies: Practice more limit calculations.\\\\n\\\\nQuestion 4:\\\\n- Question Type and Content Description: Multiple choice question on limits.\\\\n- Correct Answer: C) 6\\\\n- Student's Answer: C\\\\n- Cognitive Level: Comprehension\\\\n- Achievement Level: Mostly Achieved\\\\n- Potential Reasons: The student correctly identified the sum of limits.\\\\n- Feedback and Strategies: Practice more on limit operations.\\\\n\\\\nQuestion 5:\\\\n- Question Type and Content Description: Text answer question on limits.\\\\n- Correct Answer: A function might not be defined at a point if there's a hole or it jumps to a different value. But you can still see where it's headed from both sides, which is why the limit exists.\\\\n- Student's Answer: A function might not be defined at a point if there's a hole or it jumps to a different value. But you can still see where it's headed from both sides, which is why the limit exists.\\\\n- Cognitive Level: Synthesis/Creation\\\\n- Achievement Level: Achieved\\\\n- Potential Reasons: The student provided a clear and accurate explanation.\\\\n- Feedback and Strategies: Encourage critical thinking in explaining mathematical concepts.\\\\n\\\\nJSON Object:\\\\n\\`\\`\\`json\\\\n{\\\\n \\\\\"Question 1\\\\\": {\\\\n \\\\\"Score\\\\\": 10,\\\\n \\\\\"Cognitive Level\\\\\": \\\\\"Knowledge\\\\\"\\\\n },\\\\n \\\\\"Question 2\\\\\": {\\\\n \\\\\"Score\\\\\": 5,\\\\n \\\\\"Cognitive Level\\\\\": \\\\\"Comprehension\\\\\"\\\\n },\\\\n \\\\\"Question 3\\\\\": {\\\\n \\\\\"Score\\\\\": 7,\\\\n \\\\\"Cognitive Level\\\\\": \\\\\"Application\\\\\"\\\\n },\\\\n \\\\\"Question 4\\\\\": {\\\\n \\\\\"Score\\\\\": 10,\\\\n \\\\\"Cognitive Level\\\\\": \\\\\"Comprehension\\\\\"\\\\n },\\\\n \\\\\"Question 5\\\\\": {\\\\n \\\\\"Score\\\\\": 9,\\\\n \\\\\"Cognitive Level\\\\\": \\\\\"Synthesis/Creation\\\\\"\\\\n }\\\\n}\\\\n\\`\\`\\`\"\n",
      "\n",
      "]\n",
      "\n",
      "{\n",
      "\n",
      "\"Question 1\": {\n",
      "\n",
      "\"Score\": 10,\n",
      "\n",
      "\"Cognitive Level\": \"Knowledge\"\n",
      "\n",
      "},\n",
      "\n",
      "\"Question 2\": {\n",
      "\n",
      "\"Score\": 5,\n",
      "\n",
      "\"Cognitive Level\": \"Comprehension\"\n",
      "\n",
      "},\n",
      "\n",
      "\"Question 3\": {\n",
      "\n",
      "\"Score\": 7,\n",
      "\n",
      "\"Cognitive Level\": \"Application\"\n",
      "\n",
      "},\n",
      "\n",
      "\"Question 4\": {\n",
      "\n",
      "\"Score\": 10,\n",
      "\n",
      "\"Cognitive Level\": \"Comprehension\"\n",
      "\n",
      "},\n",
      "\n",
      "\"Question 5\": {\n",
      "\n",
      "\"Score\": 9,\n",
      "\n",
      "\"Cognitive Level\": \"Synthesis/Creation\"\n",
      "\n",
      "}\n",
      "\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"Questions\"\n",
    "retireved_results=db.similarity_search(query)\n",
    "print(retireved_results[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# web based loader\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "\n",
    "## load,chunk and index the content of the html page\n",
    "\n",
    "loader=WebBaseLoader(web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "                     bs_kwargs=dict(parse_only=bs4.SoupStrainer(\n",
    "                         class_=(\"post-title\",\"post-content\",\"post-header\")\n",
    "\n",
    "                     )))\n",
    "\n",
    "text_documents=loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pdf reader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader=PyPDFLoader('test.pdf')\n",
    "docs=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='RepairAgent: An Autonomous, LLM-Based\\nAgent for Program Repair\\nIslem Bouzenia\\nUniversity of Stuttgart\\nGermany\\nfibouzenia@esi.dzPremkumar Devanbu\\nUC Davis\\nUSA\\nptdevanbu@ucdavis.eduMichael Pradel\\nUniversity of Stuttgart\\nGermany\\nmichael@binaervarianz.de\\nAbstract —Automated program repair has emerged as\\na powerful technique to mitigate the impact of software\\nbugs on system reliability and user experience. This paper\\nintroduces RepairAgent, the first work to address the pro-\\ngram repair challenge through an autonomous agent based\\non a large language model (LLM). Unlike existing deep\\nlearning-based approaches, which prompt a model with a\\nfixed prompt or in a fixed feedback loop, our work treats\\nthe LLM as an agent capable of autonomously planning\\nand executing actions to fix bugs by invoking suitable tools.\\nRepairAgent freely interleaves gathering information about\\nthe bug, gathering repair ingredients, and validating fixes,\\nwhile deciding which tools to invoke based on the gathered\\ninformation and feedback from previous fix attempts. Key\\ncontributions that enable RepairAgent include a set of\\ntools that are useful for program repair, a dynamically\\nupdated prompt format that allows the LLM to interact\\nwith these tools, and a finite state machine that guides the\\nagent in invoking the tools. Our evaluation on the popular\\nDefects4J dataset demonstrates RepairAgent’s effectiveness\\nin autonomously repairing 164 bugs, including 39 bugs\\nnot fixed by prior techniques. Interacting with the LLM\\nimposes an average cost of 270,000 tokens per bug, which,\\nunder the current pricing of OpenAI’s GPT-3.5 model,\\ntranslates to 14 cents per bug. To the best of our knowledge,\\nthis work is the first to present an autonomous, LLM-based\\nagent for program repair, paving the way for future agent-\\nbased techniques in software engineering.\\nI. I NTRODUCTION\\nSoftware bugs lead to system failures, security vul-\\nnerabilities, and compromised user experience. Fixing\\nbugs is a critical task in software development, but\\nif done manually, demands considerable time and ef-\\nfort. Automated program repair (APR) promises to dra-\\nmatically reduce this effort by addressing the critical\\nneed for effective and efficient bug resolution in an\\nautomated manner. Researchers and practitioners have\\nexplored various approaches to address the challenge of\\nautomatically fixing bugs [1], including techniques based\\non manually designed [2], [3] and (semi-)automatically\\nextracted [4], [5], [6] fix patterns, based on symbolic\\nconstraints [7], [8], [9], and various machine learning-\\nbased approaches [10], [11], [12], [13], [14], [15], [16].The current state-of-the-art in APR predominantly\\nrevolves around large language models (LLMs). The first\\ngeneration of LLM-based repair techniques involve a\\none-time interaction with the model, where the model re-\\nceives a prompt containing the buggy code and produces\\na fixed version [17], [18]. The second and current genera-\\ntion of LLM-based repair techniques introduces iterative\\napproaches, which query the LLM repeatedly based on\\nfeedback obtained from previous fix attempts [19], [20],\\n[21].\\nA key limitation of current iterative, LLM-based repair\\ntechniques is that their hard-coded feedback loops do\\nnot allow the model to gather information about the\\nbug or existing code that may provide ingredients to\\nfix the bug. Instead, these approaches fix the context\\ninformation provided in the prompt, typically to the\\nbuggy code [19], [21], and sometimes also details about\\nthe test cases that fail [20]. The feedback loop then\\nexecutes the tests on different variants of the buggy code\\nand adds any compilation errors, test failures, or other\\noutput, to the prompt of the next iteration. However,\\nthis approach fundamentally differs from the way human\\ndevelopers fix bugs, which typically involves a temporal\\ninterleaving of gathering information to understand the\\nbug, searching code that may be helpful for fixing the\\nbug, and experimenting with candidate fixes [22], [23].\\nThis paper presents RepairAgent, the first au-\\ntonomous, LLM-based agent for automated program\\nrepair. Our approach treats the LLM as an autonomous\\nagent capable of planning and executing actions to\\nachieve the goal of fixing a bug. To this end, we equip\\nthe LLM with a set of bug repair-specific tools that\\nthe models can invoke to interact with the code base\\nin a way similar to a human developer. For example,\\nRepairAgent has tools to extract information about the\\nbug by reading specific lines of code, to gather repair\\ningredients by searching the code base, and to propose\\nand validate fixes by applying a patch and executing test\\ncases. Importantly, we do not hard-code how and when to\\nuse these tools, but instead let the LLM autonomouslyarXiv:2403.17134v1  [cs.SE]  25 Mar 2024', metadata={'source': 'test.pdf', 'page': 0}),\n",
       " Document(page_content='decide which tool to invoke next, based on previously\\ngathered information and feedback from previous fix\\nattempts.\\nOur approach is enabled by three key components.\\nFirst, a general-purpose LLM, such as GPT-3.5, which\\nwe query repeatedly with a dynamically updated prompt.\\nWe contribute a novel prompt format that guides the\\nLLM through the bug repair process, and that gets\\nupdated based on the commands invoked by the LLM\\nand the results of the previous command executions.\\nSecond, a set of tools that the LLM can invoke to interact\\nwith the code base. We present a set of 14 tools designed\\nto cover different steps a human developer would take\\nwhen fixing a bug, such as reading specific lines of code,\\nsearching the code base, and applying a patch. Third, a\\nmiddleware that orchestrates the communication between\\nthe LLM and the tools. We present novel techniques for\\nguiding tool invocations through a finite state machine\\nand for heuristically interpreting possibly incorrect LLM\\noutputs. The iterative loop of RepairAgent continues\\nuntil the agent declares to have found a suitable fix, or\\nuntil exhausting a budget of iterations.\\nTo evaluate the effectiveness of our approach, we\\napply it to all 835 bugs in the Defects4J [24] dataset,\\na widely used benchmark for evaluating program repair\\ntechniques. RepairAgent successfully fixes 164 bugs,\\nincluding 74 and 90 bugs of Defects4J v1.2 and v2.0,\\nrespectively. The correctly fixed bugs include 49 bugs\\nthat require fixing more than one line, showing that\\nRepairAgent is capable of fixing complex bugs. Com-\\npared to state-of-the-art techniques [19], [21], Repair-\\nAgent successfully fixes 39 bugs not fixed by prior work.\\nMeasuring the costs imposed by interacting with the\\nLLM, we find that RepairAgent imposes an average cost\\nof 270,000 tokens per bug, which, under the current\\npricing of OpenAI’s GPT-3.5 model, translates to 14\\ncents per bug. Overall, our results show that our agent-\\nbased approach establishes a new state of the art in\\nprogram repair.\\nIn summary, this paper contributes the following:\\n•An autonomous, LLM-based agent for program repair.\\n•A dynamically updated prompt format that guides the\\nLLM through the bug repair process.\\n•A set of tools that enable a LLM to to perform steps\\na human developer would take when fixing a bug.\\n•A middleware that orchestrates the communication\\nbetween the LLM and the tools.\\n•Empirical evidence that RepairAgent establishes a\\nnew state of the art by successfully fixing 164 bugs,\\nincluding 39 bugs not fixed by prior work.\\n•We will release the implementation of RepairAgent as\\nopen-source to foster future work.\\nTo the best of our knowledge, there currently is no\\npublished work on an autonomous, LLM-based agentfor any code-generation task. We envision RepairAgent\\nto pave the way for future agent-based techniques in\\nsoftware engineering.\\nII. B ACKGROUND ON LLM-B ASED , AUTONOMOUS\\nAGENTS\\nBy virtue of being trained on vast amounts of web\\nknowledge, including natural language and source code,\\nLLMs have demonstrated remarkable abilities in achiev-\\ning human-level performance for various tasks [25]. A\\npromising way of using these abilities are LLM-based\\nagents that autonomously plan and execute actions to\\nachieve a goal. The basic idea is to query the LLM with\\na prompt that contains the current state of the world, the\\ngoal to be achieved, and a set of actions that could be\\nperformed next. The model than decides which action to\\nperform, and the feedback from performing the action\\nis integrated into the next prompt. One way to represent\\n“actions” is through tools that the model can invoke to\\ninteract with the world [26], [27]. Recent surveys provide\\na comprehensive overview of LLM-based, autonomous\\nagents [28] and of LLM agents equipped with tools\\ninvoked via APIs [29]. The potential of such agents\\nfor software engineering currently is not well explored,\\nwhich this paper aims to address for the challenging task\\nof automated program repair.\\nIII. A PPROACH\\nA. Overview\\nFigure 1 gives an overview of the RepairAgent ap-\\nproach, which consists of three components: an LLM\\nagent, a set of tools, and a middleware that orchestrates\\nthe communication between the two. Given a bug to fix,\\nthe middleware initializes the LLM agent with a prompt\\nthat contains task information and instructions on how\\nto perform it by using the provided tools. The LLM\\nresponds by suggesting a call to one of the available\\ntools, which the middleware parses and then executes.\\nThe output of the tool is then integrated into the prompt\\nfor the next invocation of the LLM, and the process\\ncontinues iteratively until the bug is fixed or a predefined\\nbudget is exhausted.\\nB. Terminology\\nRepairAgent proceeds in multiple iterations, which we\\ncall cycles:\\nDefinition 1 (Cycle) .Acycle represents one round of\\ninteraction with the LLM agent, which consists of the\\nfollowing steps:\\n1) Query the agent\\n2) Post-process the response\\n3) Execute the command suggested by the agent\\n4) Update the dynamic prompt based on the command’s\\noutput', metadata={'source': 'test.pdf', 'page': 1}),\n",
       " Document(page_content='LLM agent Tools Middleware\\n User\\nRead code\\nSearch code base\\nRun tests\\nState and discard\\nhypotheses\\nWrite a patchInvoke command\\nCommand to\\nexecute nextQuery agent with\\ndynamic prompt\\nRaw tool outputGuide agent via a\\nstate machine\\nParse and refine\\nLLM output\\nStore and summarize\\ntool outputRepairAgent\\nFixBugFig. 1: Overview of RepairAgent.\\nTABLE I: Sections of the dynamically updated prompt.\\nPrompt section Nature\\nRole Static\\nGoals Static\\nGuidelines Static\\nState description Dynamic\\nAvailable tools Dynamic\\nGathered information Dynamic\\nSpecification of output format Static\\nLast executed command and result Dynamic\\nIn each cycle, the approach queries the LLM once.\\nThe input to the model is updated based on commands\\n(calls to tools) invoked by the LLM, and their results,\\nin previous cycles. We call the model input a dynamic\\nprompt:\\nDefinition 2 (Dynamic prompt) .The dynamic prompt\\nis a sequence of text sections P= [s0, s1, ..., s n], where\\neach section siis one of the following (where si(c)refers\\nto a section during a cycle c):\\n•Astatic section , which remains the same across all\\ncycles, i.e., si(c) =si(c′)for all c, c′.\\n•Adynamic section , which may differ across cycles,\\ni.e., there may exist c, c′withsi(c) ̸=si(c′).\\nC. Dynamic Prompting of the Repair Agent\\nThe repair agent is an LLM trained on natural lan-\\nguage and source code, such as GPT-3.5. RepairAgent\\nqueries the LLM with a dynamic prompt that consists\\nof a sequence of static and dynamic sections, as listed\\nin Table I. We describe each section in detail in the\\nfollowing.\\n1) Role: This section of the prompt defines the\\nagent’s area of expertise, which is to resolve bugs in\\nJava code, and outlines the agent’s primary objective:\\nunderstanding and fixing bugs. The prompt emphasizes\\nthat the agent’s decision-making process is autonomous\\nand should not rely on user assistance.\\n2) Goals: We define five goals for the agent to pursue,\\nwhich remain the same across all cycles:\\n•Locate the bug: Execute tests and use fault localiza-\\ntion techniques to pinpoint the bug’s location. Skip\\nthis goal when fault localization information is already\\nprovided in the prompt.•Gather information about the bug: Analyze the lines\\nof code associated with the bug to understand the bug.\\n•Suggest simple fixes to the bug: Start by suggesting\\nsimple fixes.\\n•Suggest complex fixes: If simple fixes prove ineffec-\\ntive, explore and propose more complex ones.\\n•Iterate over the previous goals: Continue to gather\\ninformation and to suggest fixes until finding a fix.\\n3) Guidelines: We provide a set of guidelines. First,\\nwe inform the model that there are diverse kinds of\\nbugs, ranging from single-line issues to multi-line bugs\\nthat may entail changing, removing, or adding lines.\\nBased on the observation that many bugs can be fixed by\\nrelatively simple, recurring fix patterns [30], we provide\\na list of recurring fix patterns. The list is based on\\nthe patterns described in prior work on single-statement\\nbugs in Java [30]. For each pattern, we provide a short\\nnatural language description and an example of buggy\\nand fixed code. Second, we instruct the model to insert\\ncomments above the modified code, which serves two\\npurposes. On the one hand, the comments allow the\\nmodel to explain its reasoning, which has been shown\\nto enhance the reasoning abilities of LLMs [31]. On\\nthe other hand, commenting will ultimately help human\\ndevelopers in understanding the nature of the edits.\\nThird, we instruct the model to conclude its reasoning\\nwith a clearly defined next step that can be translated into\\na call to a tool. Finally, we describe that there is a limited\\nbudget of tool invocations, highlighting the importance\\nof efficiency in selecting the next steps. Specifically, we\\nspecify a maximum number of cycles (40 by default).\\n4) State Description: To guide the LLM agent toward\\nusing the available tools in an effective and meaningful\\nway, we define a finite state machine that constrains\\nwhich tools are available at a given point in time.\\nThe motivation is that we observed the LLM agent\\nto frequently get lost in aimless exploration in earlier\\nexperiments without such guidance. Figure 2 shows the\\nfinite state machine, which we design to mimic the states\\na human developer would go through when fixing a bug.\\nEach state is associated with a set of tools available to the\\nagent, which are described in Section III-D. Importantly,\\nthe agent is free to transition between states at any\\npoint in time by using tools. That is, despite providing', metadata={'source': 'test.pdf', 'page': 2}),\n",
       " Document(page_content='Try to fix\\nthe bugCollect\\ninformation\\nto fix the\\nbugUnderstand\\nthe bugrun_tests\\nextract_tests\\nrun_fault_localization\\nexpress_hypothesissearch_code_base\\nfind_similar_api_calls\\ngenerate_method_body\\nread_range\\nget_classes_and_methods\\nextract_method\\nDone\\nwrite_fix\\nread_rangediscard_hypothesisdiscard_hypothesis\\nwrite_fixcollect_more_information\\ngoal_accomplishedFig. 2: State machine to guide selection of tools.\\nguidance, the state machine does not enforce a strict\\norder of tool invocations.\\nThe state description section of the prompt informs\\nthe agent about its current state:\\n•Understand the bug : The agent starts in this state,\\nwhere it can collect information related to the failing\\ntest cases and the bug’s location. Once the agent\\nhas an understanding of the bug, it formulates a\\nhypothesis to describe the nature of the bug and the\\nreason behind it. Throughout the repair process, the\\nagent may refute earlier hypotheses and express new\\nones. After expressing a hypothesis, the agent will\\nautomatically switch to the next state.\\n•Collect information to fix the bug : In this state the\\nagent collects information that help suggest a fix for\\nthe bug expressed by the hypothesis, e.g., by searching\\nfor specific repair ingredients or by reading possibly\\nrelevant code. Once the agent has gathered enough\\ninformation to attempt a fix, it can transition to the\\nnext state.\\n•Try to fix the bug : In this state, the agent tries to\\nfix the bug based on its current hypothesis and the\\ncollected information. Each fix attempt modifies the\\ncode base and is validated by executing the test cases.\\nIf necessarily, the agent can go back to one of the\\nprevious states to establish a new hypothesis or to\\ngather additional information.\\nIn addition to the three above states, RepairAgent has a\\nfinal state, “Done” , which the agent can reach by calling\\na specific command that indicates the success of repair.\\n5) Available Tools: This section of the prompt de-\\nscribes a set of tools that the agent can call at the current\\nstate. Each tool has a name, a description, and a set ofinterface Response {\\nthoughts :string;\\ncommand :{\\nname :string;\\nargs :Record<string, any>;\\n};\\n}\\nFig. 3: JSON format of the response of the model.\\n{\\n\"thoughts\" :\"With the hypothesis in mind,\\nit’s time to gather more information to\\nformulate potential fixes for the bug. I\\nshould focus on understanding the context\\naround the condition\\n’if (x < 0 && prev == ’-’)’\\nto come up with effective solutions.\",\\n\"command\" :{\\n\"name\" :\"search_code_base\",\\n\"args\" :{\\n\"key_words\" :\\n[\"addNumber\",\\n\"CodeConsumer\",\\n\"if (x < 0 && prev == ’-’)\" ]\\n}}}\\nFig. 4: Example of a response of the repair agent.\\ntyped arguments (Section III-D).\\n6) Gathered Information: A key ability of the repair\\nagent is to gather information about the bug and the code\\nbase, which serves as the basis for deciding which com-\\nmands to invoke next. To make this information available\\nto the agent, we maintain a prompt section that lists the\\ninformation gathered by the different tool invocations.\\nIntuitively, this section of the prompt serves as a memory\\nfor the agent, allowing it to recall information from pre-\\nvious cycles. The gathered information is structured into\\ndifferent subsections, where each subsection contains the\\noutputs produced by a specific tool.\\n7) Specification of Output Format: Given the dy-\\nnamic prompt, the LLM agent provides one response\\nper cycle. To enable the middleware to parse the re-\\nsponse, we specify the expected output format (Figure 3).\\nThe “thoughts” field provides a textual description of\\nthe agent’s reasoning when deciding about the next\\ncommand. Asking the agent to express its thoughts\\nincreases the transparency and interpretability of the\\napproach, provides a way to debug potential issues in the\\nagent’s decision-making process, and helps improve the\\nreasoning abilities of LLMs [31]. The “command” field\\nspecifies the next command to be executed, consisting of\\nthe name of the tool to invoke and the set of arguments.\\nFor example, Figure 4 shows a response of the LLM\\nagent. The model expresses the need to collect more\\ninformation to understand the bug. It then suggests a\\ncommand that searches the code base with a list of\\nkeywords.', metadata={'source': 'test.pdf', 'page': 3}),\n",
       " Document(page_content='8) Last Executed Command and Result: This section\\nof the prompt contains the last command (tool name and\\narguments) that was executed (if any) and the output it\\nproduced. The rationale is to remind the agent of the\\nlast step it took, and to make it aware of any problems\\nthat occurred during the execution of the command.\\nFurthermore, we remind the agent how many cycles have\\nalready been executed, and how many cycles are left\\nD. Tools for the Agent to Use\\nA key novelty in our approach is to let an LLM agent\\nautonomously decide which tools to invoke to fix a bug.\\nThe tools we provide to the agent (Table II) are inspired\\nby the tools that developers use in their IDEs.\\n1) Reading and Extracting Code: A prerequisite for\\nfixing a bug is to read and understand relevant parts\\nof the code base. Instead of hard-coding the context\\nprovided to the LLM [19], [20], [21], we let the agent\\ndecide which parts of the code to read, based on four\\ntools. The read range tool allows the agent to extract\\na range of lines from a specific file, which is useful\\nto obtain a focused view of a particular section of\\ncode. To obtain an overview of the code structure,\\nthegetclasses and methods tool retrieves all class and\\nmethod names within a given file. By invoking the\\nextract method tool, the agent can retrieve the imple-\\nmentation(s) of methods that match a given method name\\nwithin a given file. Finally, we offer the extract tests\\ntool, which extracts the code of test cases that resulted in\\nfailure. The tool is crucial to understand details of failing\\ntests, such as input values and the expected output.\\n2) Search and generate code: Motivated by the fact\\nthat human developers commonly search for code [32],\\nwe present tools that allow the agent to search for\\nspecific code snippets. These tools are useful for the\\nagent to better understand the context of a bug and\\nto gather repair ingredients, i.e., code fragments that\\ncould become part of a fix. The search code base tool\\nenables the agent to locate instances of particular key-\\nwords within the entire code base. For example, the\\nagent can use this tool to find occurrences of vari-\\nables, methods, and classes. Given a set of keywords,\\nthe tool performs an approximate matching against all\\nsource code files in the project. Specifically, the tool\\nsplits each keyword into subtokens based on camel\\ncase, underscores, and periods, and then searches for\\neach subtoken in the code. For example, searching\\nforquickSortArray yields matches for sortArray ,\\nquickSort ,arrayQuickSort , and other related vari-\\nations. The output of the tool is a nested dictionary,\\norganized by file names, classes, and method names, that\\nprovides the keywords that match a method’s content.\\nAnother search tool, find similar apicalls , allows the\\nagent to identify and extract usages of a method, whichis useful to fix incorrect method calls. Without such a\\ntool, LLMs tend to hallucinate method calls that do not\\nexist in the code base [33]. Given a code snippet that\\ncontains a method call, the tool extracts the name of the\\ncalled method, and then searches for calls to methods\\nwith the same name. The agent can restrict the search to\\na specific file or search the entire code base.\\nIn addition to searching for existing code, Repair-\\nAgent offers a tool that generates new code by invoking\\nanother LLM. The tool is inspired by the success of\\nLLM-based code completion tools, such as Copilot [34],\\nwhich human developers increasingly use when fixing\\nbugs. Given the code preceding a method and the signa-\\nture of the method, the generate method body tool asks\\nan LLM to generate the body of the method. The query to\\nthe code-generating LLM is independent of the dynamic\\nprompt used by the overall RepairAgent approach, and\\nmay use a different model. In our evaluation, we use the\\nsame LLM for both the repair agent and as the code-\\ngenerating LLM of this tool. The tool limits the given\\ncode context to 12k tokens and sets a limit of 4k tokens\\nfor the generated code.\\n3) Testing and Patching: The next category of tools\\nis related to running tests and applying patches. The\\nrun tests tool allows the agent to execute the test suite\\nof the project. It produces a report that indicates whether\\nthe tests passed or failed. In case of test failures, the tool\\ncleans the output of the test runner, e.g., by removing\\nentries of the stack trace that are outside of the current\\nproject. The rationale is that LLMs have a limited prompt\\nsize and that irrelevant information may confuse the\\nmodel. The run fault localization tool retrieves fault\\nlocalization information, which is useful to understand\\nwhich parts of the code are likely to contain the bug.\\nRepairAgent offers two variants of this tool: Either, it\\nprovides perfect fault localization information, i.e., the\\nfile(s) and line(s) that need to be edited to fix the bug,\\nor it invokes an existing fault localization tool, such\\nas GZoltar [35], to calculate fault localization scores.\\nAs common in the field of program repair, we assume\\nperfect fault localization as the default.\\nOnce the agent has gathered sufficient information to\\nfix the bug, it can apply a patch to the code base using the\\nwrite fixtool. RepairAgent aims at repairing arbitrarily\\ncomplex bugs, including multi-line and even multi-file\\nbugs. The write fixtool expects a patch in a specific\\nJSON format, which indicates the insertions, deletions,\\nand modifications to be made in each file. Figure 5 shows\\nan example of a patch in this format. Given a patch,\\nthe tool applies the changes to the code base and runs\\nthe test suite. If the tests fail, the write fixreverts the\\nchanges, giving the agent a clean code base to try another\\nfix. Motivated by the observation that some fix attempts\\nare almost correct, the write fixtool requests the LLM', metadata={'source': 'test.pdf', 'page': 4}),\n",
       " Document(page_content='TABLE II: Repair-related tools invoked by RepairAgent.\\nTool Description\\nRead and extract code:\\nread range Read a range of lines in a file.\\ngetclasses and methods Get the names of all classes and methods in a file.\\nextract method Given a method name, extract method implementations from a file.\\nextract tests Given the failure report from JUnit or ANT, extract the code of failing test cases.\\nSearch and generate code:\\nsearch code base Scans all Java files within a project for a list of keywords.\\nfind similar api calls Given a code snippet that calls an API, search for similar API calls in the project.\\ngenerate method body Ask an LLM (GPT3.5 by default) to generate the body of a method based on code proceeding the method.\\nTesting and patching:\\nrun tests Run the test suite of a project.\\nrun fault localization Retrieve pre-existing localization information or run a fault localization tool.\\nwrite fix Apply a patch to the code base and execute the test suite of the project. Changes are reverted automatically if tests\\nfail. Moves the agent into the ’Try to fix the bug’ state.\\nControl:\\nexpress hypothesis Express a hypothesis about the bug. Moves the agent into the ’Collect information to fix the bug’ state.\\ncollect more information Move the agent back to the ’Collect information to fix the bug’ state.\\ndiscard hypothesis Discard the current hypothesis about the bug and move back to the ’Understand the bug’ state.\\ngoal accomplished Declare that the goal has been accomplished and exiting the repair process.\\n[\\n{\\n\"file_path\" :\"jfree/data/time/Week.java\",\\n\"insertions\" : [\\n{\\n\"line_number\" :175,\\n\"new_lines\" : [\\n\"// ...new lines to insert...\\\\n\",\\n\"// ...more new lines...\\\\n\" ]\\n}\\n],\\n\"deletions\" : [179, 183 ],\\n\"modifications\" : [\\n{\\n\"line_number\" :179,\\n\"modified_line\" :\" if (dataset == null\\n) {\\\\n\"\\n}\\n]\\n},\\n{\\n\"file_path\" :\"org/jfree/data/time/Day.java\"\\n,\\n\"insertions\" : [] ,\\n\"deletions\" : [307],\\n\"modifications\" : []\\n}\\n]\\nFig. 5: Example of patch given to the write fixtool.\\nto sample multiple variants of the suggested fix. By\\ndefault, RepairAgent samples 30 variants at max. Given\\nthe generated variants, the approach removes duplicates\\nand launch tests for every variant.\\n4) Control: The final set of tools do not directly cor-\\nrespond to a tool a human developer may use, but rather\\nallow the agent to move between states (Figure 2). The\\nexpress hypothesis tool empowers the agent to articulatea hypothesis regarding the nature of the bug and to\\ntransition to the ’Collect information to fix the bug’ state.\\nInversely, the discard hypothesis tool allows the agent\\nto discard a hypothesis that is no longer viable, which\\nleads back to the ’Understand the bug’ state. Together,\\nthe two commands enforce a structured approach to\\nhypothesis formulation, aligning with work on scientific\\ndebugging [36], [20]. In case the agent has tried multiple\\nfixes without success, the collect more information tool\\nallows the agent to revert to the ’Collect information\\nto fix the bug’ state. Finally, once the agent has found\\nat least one fix that passes all tests, it can invoke the\\ngoal accomplished tool, which terminates RepairAgent.\\nE. Middleware\\nThe middleware component plays a crucial role in\\nRepairAgent, orchestrating the communication between\\nthe LLM agent and the tools. It performs the steps in\\nDefinition 1 as described in the following.\\n1) Parsing and Refining LLM Output: At the begin-\\nning of each cycle, the middleware queries the LLM\\nwith the current prompt. Ideally, the response adheres\\nperfectly to the expected format (Figure 3). In practice,\\nhowever, the LLM may produce responses that deviate\\nfrom the expected format, e.g., due to hallucinations or\\nsyntactic errors. For example, the LLM may provide\\na “path” argument while the tool expects a “file path”\\nargument.\\nRepairAgent tries to heuristically rectify such issues\\nby mapping the output to the expected format in three\\nsteps. First, it tries to map the tool mentioned in the\\nresponse to one of the available tools. Specifically, the\\napproach checks if the predicted tool name npredicted is', metadata={'source': 'test.pdf', 'page': 5}),\n",
       " Document(page_content='a substring of the name of any available tool nactual , or\\nvice versa, and if yes, considers nactual to be the desired\\ntool. In case the above matching fails, the approach\\nchecks if the Levenshtein distance between npredicted\\nand any nactual is below a threshold (0.1 by default).\\nSecond, the approach tries to map the argument names\\nprovided in the response to the expected arguments of the\\ntool, following the same logic as above. Third, the ap-\\nproach handles invalid argument values by heuristically\\nmapping or replacing them, e.g., by replacing a predicted\\nfile path with a valid one. If the heuristic mapping\\nfails or produces multiple possible tool invocations, the\\nmiddleware informs the LLM about the issue via the\\n“Last executed command and result” prompt section and\\nenters a new cycle.\\nIn addition to rectifying minor mistakes in the re-\\nsponse, the middleware also checks for repeated invo-\\ncations of the same tool with the same arguments. If\\nthe agent suggests the exact same command as in a\\nprevious cycle, which would yield the same results, the\\nmiddleware informs the agent about the repetition and\\nenters a new cycle.\\n2) Calling the Tool: Once the middleware has re-\\nceived a valid command from the LLM, it calls the\\ncorresponding tool. To prevent tool executions to inter-\\nfere with the host environment or RepairAgent itself,\\nthe middleware executes the command in an isolated\\nenvironment that contains a cloned version of the buggy\\nrepository.\\n3) Updating the Prompt: Given the output of the\\ntool, the middleware updates all dynamic sections of the\\nprompt for the next cycle. In particular, the middleware\\nupdates the state description and the available tools,\\nappends the tool’s output to the gathered information,\\nand replaces the section that shows the last executed\\ncommand.\\nIV. I MPLEMENTATION\\nWe use Python 3.10 as our primary programming\\nlanguage. Docker is used to containerize and isolate\\ncommand executions for enhanced reliability and repro-\\nducibility. RepairAgent is built on top of the AutoGPT\\nframework and GPT-3.5-0125 from OpenAI. To parse\\nand interact with Java code, we use ANTLR.\\nV. E VALUATION\\nTo evaluate our approach we aim to answer the\\nfollowing research questions:\\nRQ1 How effective is RepairAgent at fixing real-world\\nbugs?\\nRQ2 What are the costs of the approach?\\nRQ3 How does the LLM agent use the available tools?TABLE III: Results on Defects4J.\\nProject Bugs Plausible Correct ChatRepair ITER SelfAPR\\nChart 26 14 11 15 10 7\\nCli 39 9 8 5 6 8\\nClosure 174 27 27 37 18 20\\nCodec 18 10 9 8 3 8\\nCollections 4 1 1 0 0 1\\nCompress 47 10 10 2 4 7\\nCsv 16 6 6 3 2 1\\nGson 18 3 3 3 0 1\\nJacksonCore 26 5 5 3 3 3\\nJacksondatabind 112 18 11 9 0 8\\nJacksonXml 6 1 1 1 0 1\\nJsoup 93 18 18 14 0 6\\nJxPath 22 0 0 0 0 1\\nLang 63 17 17 21 0 10\\nMath 106 29 29 32 0 22\\nMockito 38 6 6 6 0 3\\nTime 26 3 2 3 2 3\\nDefects4Jv1.2 395 96 74 114 57 64\\nDefects4Jv2 440 90 90 48 — 46\\nTotal 835 186 164 162 57 110\\nA. Experimental Setup\\na) Dataset: We apply RepairAgent to bugs in the\\nDefects4J dataset [24]. We use the entire Defects4J\\ndataset, which consists of 835 real-world bugs from\\n17 Java projects, including 395 bugs from 6 projects\\nin Defects4Jv1.2, as well as another 440 bugs and 11\\nprojects added in Defects4Jv2. Evaluating on the entire\\ndataset allows us to assess the generalization capabilities\\nof RepairAgent to different projects and bugs, without\\nrestricting the evaluation, e.g., based on the number of\\nlines, hunks, or files that need to be fixed.\\nb) Baselines: We compare with three existing re-\\npair techniques: ChatRepair [19], ITER [21], and Self-\\nAPR [37]. ChatRepair and ITER are two very recent ap-\\nproaches and have been shown to be the current state of\\nthe art. All three baseline approaches follow an iterative\\napproach that incorporates feedback from previous patch\\nattempts. Unlike RepairAgent, the baselines do not use\\nan autonomous, LLM-based agent. We compare against\\nthe baselines based on patches provided by the authors\\nof the respective approaches.\\nc) Metrics of success: Similar to past work, we\\nreport both the number of plausible and correct patches.\\nA fix is plausible if it passes all test cases, but is not\\nnecessarily correct. To determine whether a fix is correct,\\nwe automatically check whether it syntactically matches\\nthe developer-created fix. If this is not the case, we\\nmanually determine whether the RepairAgent-generated\\nfix is semantically consistent with the developer-created\\nfix. If and only if either of the two checks succeeds, we\\nconsider the fix to be correct .\\nB. RQ1: Effectiveness\\n1) Overall Results: Table III summarizes the ef-\\nfectiveness of RepairAgent in fixing the 835 bugs in', metadata={'source': 'test.pdf', 'page': 6}),\n",
       " Document(page_content='TABLE IV: Distribution of fixes by location type\\nBug type RepairAgent ChatRepair ITER SelfAPR\\nSingle-line 115 133 36 83\\nMulti-line* 46 29 14 24\\nMulti-file 3 0 4 3\\nFig. 6: Intersection of the set fixes with related work.\\nDefects4J. The approach generates plausible fixes for\\n186 bugs. While not necessarily correct, plausible fixes\\npass all test cases and may still provide developers a\\nhint about what should be changed. RepairAgent gener-\\nates correct fixes for 164 bugs, which are semantically\\nconsistent with the developer-provided patches. Being\\nable to fix bugs from different projects shows that the\\napproach can generalize to code bases of multiple do-\\nmains. Furthermore, RepairAgent creates fixes for bugs\\nof different levels of complexity. Specifically, as shown\\nin Table IV, the approach fixes 115 single-line bugs, 46\\nmulti-line (single-file) bugs, and 3 multi-file bugs.\\n2) Comparison with Prior Work: The right-hand side\\nof Table III compares RepairAgent with the baseline\\napproaches ChatRepair, ITER, and SelfAPR. Previous\\nto this work, ChatRepair had established a new state\\nof the art in APR by fixing 162 bugs in Defects4J.\\nRepairAgent achieves a comparable record by fixing\\na total of 164 bugs. Our work particularly excels in\\nDefects4Jv2, where RepairAgent fixes 90 bugs, while\\nChatRepair only fixes 48 bugs. To further compare the\\nsets of fixed bugs, Figure 6 shows the overlaps between\\ndifferent approaches. As often observed in the field of\\nAPR, different approaches complement each other to\\nsome extent. In particular, RepairAgent fixes 39 bugs that\\nwere not fixed by any of the three baselines. Comparing\\nthe complexity of the bug fixes, as shown on the right-\\nhand side of Table IV, RepairAgent is particularly more\\neffective, compared to other tools, for bugs that require\\nmore than a single-line fix. We attribute this result to the\\nRepairAgent’s ability to autonomously retrieve suitable\\nrepair ingredients and the fact that the agent can perform\\nedits to an arbitrary number of lines and files.if (cfa != null) {\\nfor (Node finallyNode :\\ncfa.finallyMap.get(parent)) {\\n- cfa.createEdge(fromNode, Branch.UNCOND,\\nfinallyNode);\\n+ cfa.createEdge(fromNode, Branch.ON_EX,\\nfinallyNode);}}\\nFig. 7: Closure-14, bug fixed by RepairAgent.\\nSeparator sep = (Separator)\\nelementPairs.get(0);\\n+ if (sep.iAfterParser == null &&\\nsep.iAfterPrinter == null) {\\nPeriodFormatter f =\\ntoFormatter(elementPairs.subList(2,\\nsize), notPrinter, notParser);\\nsep = sep.finish(f.getPrinter(),\\nf.getParser());\\nreturn new PeriodFormatter(sep, sep);\\n+ }\\nFig. 8: Time-27, bug fixed by RepairAgent.\\n3) Examples: Figures 7 and 8 showcase interesting\\nbugs fixed exclusively by RepairAgent. In the\\nexample of Figure 7, the agent used the tool\\nfind similar apicalls to search for calls similar\\ntocfa.createEdge(fromNode, Branch.UNCOND,\\nfinallyNode); which returns a similar call that is\\nfound in another file but passes Branch.ON_EX to the\\nmethod call instead of Branch.UNCOND . The latter was\\nused as the repair ingredient by the agent.\\nIn the second example, RepairAgent benefeted from\\nthe tool generate method body to generate the miss-\\ning if-statement which led to suggesting a correct fix\\nafterwards.\\nFrom one side, these examples illustrate the clever\\nand proper usage of available tools by the repair agent.\\nFrom the other side, it shows how useful these tools at\\nfinding repair ingredients that traditional approaches and\\nprevious work failed to find.\\nC. RQ2: Costs of the Approach\\nWe measure three kinds of costs imposed by Repair-\\nAgent: (i) Time taken to fix a bug. (ii) The number of\\ntokens consumed by queries to the LLM, which is rel-\\nevant both for commercial models, such as the GPT-3.5\\nused here, and for self-hosted models, where the number\\nof tokens determines the computational costs. (iii) The\\nmonetary costs associated with the token consumption,\\nbased on OpenAI’s pricing as of March 2024.\\nOur findings are summarized in Figure 9. The median\\ntime taken to address a bug is 920 seconds, with minimal\\nvariation between fixed (870 seconds) and unfixed bugs.\\nSurprisingly, fixed bugs do not consistently exhibit lower\\nrepair times. This is due to RepairAgent’s autonomous', metadata={'source': 'test.pdf', 'page': 7}),\n",
       " Document(page_content='(a) Time.\\n (b) Tokens/money consumption.\\nFig. 9: Distribution of cost metrics per bug (time, number of token, and monetary costs).\\nnature, where the repair process continues until the\\ngoal accomplished command is invoked or the cycles\\nbudget is exhausted. The figure shows several outliers\\nwhere bug fixing attempt takes multiple hours. Repair-\\nAgent spends 99% of the total time in tool executions,\\nmostly running tests. This cost could be reduced in the\\nfuture by executing selected test cases only.\\nAnalyzing the costs imposed by the LLM, we find a\\nmedian consumption of approximately 270,000 tokens,\\nequating to around 14 cents (US dollars). The number\\nof tokens consumed by fixed bugs (21,000) is clearly\\nlower than by unfixed bugs (315,000). This difference\\nis because the agent continues to extract additional\\ninformation for not yet fixed bugs, saturating the prompt\\nwith operations, such as reading more lines of code or\\nperforming extensive searches.\\nD. RQ3: Usage of Tools by the Agent\\nTo better understand the approach, we evaluate how\\nthe agent uses the available tools.\\n1) Adherence to Output Format: A notable challenge\\nin designing an LLM-based agent is the inherent infor-\\nmality and natural language noise present in the model’s\\noutput. We categorize the output of the model as follows:\\n•Fully parsable output: Responses that adhere to the\\nJSON format without requiring further refinement\\n(87.7%).\\n•Unparsable output: Responses that do not conform to\\nthe JSON format (2.3% of the responses).\\n•Partially parsable output: Responses in JSON format\\nbut with missing or misnamed fields requiring refine-\\nment by the middleware (9.9%).\\nA parsed output does not guarantee a correctly specified\\ncommand, which the middleware tries to rectify. At this\\nphase, the output may fall into the following categories:•Correct command name: The name of the command\\nis correct (97.9%).\\n•Non-existent command: The command could not be\\nfound or mapped to an existing one (1.4%).\\n•Mapped command: The command does not exist but\\ncan be mapped to an existing command by the mid-\\ndleware (0.7%).\\n•Correct arguments: The arguments are correct\\n(90.1%).\\n•Unrefinable arguments list: The command exists or\\nwas mapped, but the list of arguments is incomplete\\nor has incorrect names (1.9%).\\n•Refinable arguments list: The middleware successfully\\nmaps the list of arguments into a correct one (8.0%).\\nOverall, these results show that our heuristic refinement\\nof LLM outputs contributes to the effectiveness and\\nrobustness of RepairAgent.\\n2) Frequency of Tools Invocation: On average,\\nRepairAgent makes 35 calls per bug, which also cor-\\nresponds to the number of cycles. Figure 10 shows the\\nfrequency of tool invocations, where we distinguish be-\\ntween fixed (i.e., “correct”) and unfixed (i.e., “plausible”\\nonly or completely unfixed) bugs. The LLM agent uses\\nthe full range of tools, with the most frequently called\\ntool being write fix(average of 6 calls for fixed bugs\\nand 17 calls for unfixed bugs. Around 7% of write fix\\ninvocations in unfixed bugs produce plausible patches,\\ncompared to 44% in fixed bugs.\\nVI. T HREATS TO VALIDITY AND LIMITATIONS\\nWhile RepairAgent shows promising results when\\nrepairing Defects4J bugs, we acknowledge several poten-\\ntial threats to validity and inherent limitations: (i) Data\\nleakage: As we evaluate on GPT-3.5 and its training data\\nis not publicly known, the LLM may have seen parts of', metadata={'source': 'test.pdf', 'page': 8}),\n",
       " Document(page_content='Fig. 10: Frequency of tool invocations (average per bug).\\nthe Java projects during training. While we acknowl-\\nedge this risk, our approach does not solely depend on\\nknowing about a bug, but rather the ability to collect\\ninformation to fix the bug. We also note that the closest\\ncompetitor, ChatRepair, also uses GPT-3.5, and thus\\nfaces the same risk. (ii) Missing test cases: Defects4J\\nhas at least one failing test case for each bug, which\\nmay not be the case for real-world usage scenarios. It\\nwill be interesting to evaluate RepairAgent on bugs with\\nno a-priori available error-revealing test cases in future\\nwork. (iii) Fault localization: Inaccurate or imprecise\\nfault localization could lead to suboptimal repair sug-\\ngestions or incorrect diagnoses. (iv) Non-deterministic\\noutput of LLMs: The inherently non-deterministic nature\\nof LLMs may result in different outcomes between two\\nconsecutive runs of RepairAgent. The large number of\\nbugs we evaluate on mitigates this risk. Moreover, we\\nmake logs of all interactions with the LLM available to\\nensure reproducibility.\\nVII. R ELATED WORK\\na) Automated program repair: Automated program\\nrepair [1] has received significant attention. Some ap-\\nproaches address it as a search problem based on\\nmanually designed code mutation rules and fix pat-\\nterns [2], [38], [3]. Alternatively, transformation rules\\ncan be derived (semi-)automatically from human-written\\npatches [4], [5], [6]. Other approaches use symbolic\\nconstraints to derive fixes [7], [39], [8], [9], integrate\\nrepair into a static analysis that identifies bugs [40], [41],\\n[42], or replace buggy code with similar code from the\\nsame project [43]. APR has been successfully deployed\\nin industrial contexts [5], [44]. Beyond functional bugs,several techniques target other kinds of problems, such\\nas syntax errors [45], [46], [47], performance bugs [48],\\nvulnerabilities [49], type errors [50], common issues in\\ndeep learning code [51], and build errors [52].\\nb) Learning-based program repair: While early\\nwork uses machine learning to rank and select candidate\\nfixes [10], more recent work uses machine learning to\\ngenerate fixes. Approaches include neural machine trans-\\nlation models that map buggy code into fixed code [11],\\n[12], [13], [14], models that predict tree transforma-\\ntions [15], [16], neural architectures for specific kinds of\\nbugs [53], and repair-specific training regimes [54], [37].\\nWe refer to a recent survey for a more comprehensive\\ndiscussion [55]. Unlike the above work, RepairAgent and\\nthe work discussed below use a general-purpose LLM,\\ninstead of training a task-specific model.\\nc) LLM-based program repair: LLMs have moti-\\nvated researchers to apply them to program repair, e.g.,\\nin studies that explore prompts [18], [17] and in a tech-\\nnique that prompts the model with error messages [56].\\nThese approaches perform a one-time interaction with\\nthe model, where the model receives a prompt with code\\nand produces a fix. The most recent repair techniques\\nintroduce iterative approaches, which query the LLM\\nrepeatedly based on feedback obtained from previous\\nfix attempts [19], [20], [21]. RepairAgent also queries\\nthe model multiple times, but fundamentally differs by\\npursuing an agent-based approach. Section V empirically\\ncompares RepairAgent to the most closely related itera-\\ntive approaches [19], [21].\\nd) LLMs for code generation and code editing:\\nBeyond program repair, LLMs have been applied to\\na variety of other code generation and code editing\\ntasks, including code completion [34], [57], fuzzing [58],\\ngenerating and improving unit tests [59], [60], [61], [62],\\n[63], [64], multi-step code editing [65]. Unlike our work,\\nnone of these approaches uses an agent-based approach.\\ne) LLM-based agents: The idea to let LLM agents\\nautonomously plan and perform complex tasks is rel-\\natively new and has been applied to tasks outside of\\nsoftware engineering [28]. To the best of our knowledge,\\nour work is the first to apply an LLM-based agent to\\nprogram repair or any other code generation problem in\\nsoftware engineering. RepairAgent is inspired by prior\\nwork [29] on augmenting LLMs with tools invoked via\\nAPIs [26], [27] and with the ability to generate and\\nexecute code [66]. Our key contribution in applying these\\nideas to a software engineering task is to define tools that\\nare useful for program repair and a prompt format that\\nallows the LLM to interact with these tools.\\nVIII. C ONCLUSION\\nThis paper presents a pioneering technique for bug\\nrepair based on an autonomous agent powered by Large', metadata={'source': 'test.pdf', 'page': 9}),\n",
       " Document(page_content='Language Models (LLMs). Through extensive experi-\\nmentation, we validate the effectiveness and potential\\nof our approach. Further exploration and refinement of\\nautonomous agent-based techniques will help generalize\\nto more difficult and diverse types of bugs if equipped\\nwith the right tools.\\nREFERENCES\\n[1] C. Le Goues, M. Pradel, and A. Roychoudhury, “Automated\\nprogram repair,” Commun. ACM , vol. 62, no. 12, pp. 56–65,\\n2019. [Online]. Available: https://doi.org/10.1145/3318162\\n[2] C. Le Goues, T. Nguyen, S. Forrest, and W. Weimer, “Genprog:\\nA generic method for automatic software repair,” IEEE Trans.\\nSoftware Eng. , vol. 38, no. 1, pp. 54–72, 2012.\\n[3] K. Liu, A. Koyuncu, D. Kim, and T. F. Bissyand ´e,\\n“Tbar: revisiting template-based automated program repair,”\\ninProceedings of the 28th ACM SIGSOFT International\\nSymposium on Software Testing and Analysis, ISSTA 2019,\\nBeijing, China, July 15-19, 2019 , D. Zhang and A. Møller,\\nEds. ACM, 2019, pp. 31–42. [Online]. Available: https:\\n//doi.org/10.1145/3293882.3330577\\n[4] D. Kim, J. Nam, J. Song, and S. Kim, “Automatic patch gen-\\neration learned from human-written patches.” in International\\nConference on Software Engineering (ICSE) , 2013, pp. 802–811.\\n[5] J. Bader, A. Scott, M. Pradel, and S. Chandra, “Getafix:\\nLearning to fix bugs automatically,” Proc. ACM Program.\\nLang. , vol. 3, no. OOPSLA, pp. 159:1–159:27, 2019. [Online].\\nAvailable: https://doi.org/10.1145/3360585\\n[6] R. Bavishi, H. Yoshida, and M. R. Prasad, “Phoenix:\\nautomated data-driven synthesis of repairs for static analysis\\nviolations,” in ESEC/SIGSOFT FSE 2019, Tallinn, Estonia,\\nAugust 26-30, 2019 , M. Dumas, D. Pfahl, S. Apel, and\\nA. Russo, Eds. ACM, 2019, pp. 613–624. [Online]. Available:\\nhttps://doi.org/10.1145/3338906.3338952\\n[7] H. D. T. Nguyen, D. Qi, A. Roychoudhury, and S. Chandra,\\n“Semfix: program repair via semantic analysis,” in 35th Inter-\\nnational Conference on Software Engineering, ICSE ’13, San\\nFrancisco, CA, USA, May 18-26, 2013 , 2013, pp. 772–781.\\n[8] J. Xuan, M. Martinez, F. Demarco, M. Clement, S. L. Marcote,\\nT. Durieux, D. Le Berre, and M. Monperrus, “Nopol: Automatic\\nrepair of conditional statement bugs in java programs,” IEEE\\nTransactions on Software Engineering , vol. 43, no. 1, pp. 34–55,\\n2016.\\n[9] S. Mechtaev, J. Yi, and A. Roychoudhury, “Angelix: Scalable\\nmultiline program patch synthesis via symbolic analysis,” in\\nProceedings of the 38th international conference on software\\nengineering , 2016, pp. 691–701.\\n[10] F. Long and M. Rinard, “Automatic patch generation by learning\\ncorrect code,” in Proceedings of the 43rd Annual ACM SIGPLAN-\\nSIGACT Symposium on Principles of Programming Languages,\\nPOPL 2016, St. Petersburg, FL, USA, January 20 - 22, 2016 ,\\n2016, pp. 298–312.\\n[11] R. Gupta, S. Pal, A. Kanade, and S. K. Shevade, “Deepfix:\\nFixing common C language errors by deep learning,” in\\nProceedings of the Thirty-First AAAI Conference on Artificial\\nIntelligence , 2017, pp. 1345–1351. [Online]. Available: http:\\n//aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14603\\n[12] M. Tufano, J. Pantiuchina, C. Watson, G. Bavota, and\\nD. Poshyvanyk, “On learning meaningful code changes via\\nneural machine translation,” in Proceedings of the 41st\\nInternational Conference on Software Engineering, ICSE 2019,\\nMontreal, QC, Canada, May 25-31, 2019 , 2019, pp. 25–36.\\n[Online]. Available: https://dl.acm.org/citation.cfm?id=3339509\\n[13] T. Lutellier, H. V . Pham, L. Pang, Y . Li, M. Wei, and L. Tan,\\n“Coconut: combining context-aware neural translation models\\nusing ensemble for program repair,” in ISSTA ’20: 29th ACM\\nSIGSOFT Virtual Event, USA, July 18-22, 2020 , S. Khurshid\\nand C. S. Pasareanu, Eds. ACM, 2020, pp. 101–114. [Online].\\nAvailable: https://doi.org/10.1145/3395363.3397369[14] Z. Chen, S. Kommrusch, M. Tufano, L. Pouchet, D. Poshyvanyk,\\nand M. Monperrus, “SequenceR: Sequence-to-sequence learning\\nfor end-to-end program repair,” IEEE Trans. Software Eng. ,\\nvol. 47, no. 9, pp. 1943–1959, 2021. [Online]. Available:\\nhttps://doi.org/10.1109/TSE.2019.2940179\\n[15] Y . Li, S. Wang, and T. N. Nguyen, “Dlfix: Context-based code\\ntransformation learning for automated program repair,” in ICSE ,\\n2020.\\n[16] Q. Zhu, Z. Sun, Y . Xiao, W. Zhang, K. Yuan, Y . Xiong,\\nand L. Zhang, “A syntax-guided edit decoder for neural\\nprogram repair,” in ESEC/FSE ’21 Athens, Greece, August\\n23-28, 2021 , D. Spinellis, G. Gousios, M. Chechik, and M. D.\\nPenta, Eds. ACM, 2021, pp. 341–353. [Online]. Available:\\nhttps://doi.org/10.1145/3468264.3468544\\n[17] C. S. Xia, Y . Wei, and L. Zhang, “Automated program repair in\\nthe era of large pre-trained language models,” in 2023 IEEE/ACM\\n45th International Conference on Software Engineering (ICSE) ,\\n2023, pp. 1482–1494.\\n[18] N. Jiang, K. Liu, T. Lutellier, and L. Tan, “Impact of\\ncode language models on automated program repair,” in 45th\\nIEEE/ACM International Conference on Software Engineering,\\nICSE . IEEE, 2023, pp. 1430–1442. [Online]. Available:\\nhttps://doi.org/10.1109/ICSE48619.2023.00125\\n[19] C. S. Xia and L. Zhang, “Keep the conversation going: Fixing\\n162 out of 337 bugs for $0.42 each using ChatGPT,” 2023.\\n[20] S. Kang, B. Chen, S. Yoo, and J. Lou, “Explainable automated\\ndebugging via large language model-driven scientific debugging,”\\nCoRR , vol. abs/2304.02195, 2023. [Online]. Available: https:\\n//doi.org/10.48550/arXiv.2304.02195\\n[21] H. Ye and M. Monperrus, “Iter: Iterative neural repair for multi-\\nlocation patches,” in ICSE , 2024.\\n[22] A. J. Ko, B. A. Myers, M. J. Coblenz, and H. H. Aung, “An\\nexploratory study of how developers seek, relate, and collect\\nrelevant information during software maintenance tasks,” IEEE\\nTransactions on software engineering , vol. 32, no. 12, pp. 971–\\n987, 2006.\\n[23] M. B ¨ohme, E. O. Soremekun, S. Chattopadhyay, E. Ugherughe,\\nand A. Zeller, “Where is the bug and how is it fixed? an\\nexperiment with practitioners,” in Proceedings of the 2017 11th\\njoint meeting on foundations of software engineering , 2017, pp.\\n117–128.\\n[24] R. Just, D. Jalali, and M. D. Ernst, “Defects4j: a database\\nof existing faults to enable controlled testing studies for java\\nprograms,” in International Symposium on Software Testing and\\nAnalysis, ISSTA ’14, San Jose, CA, USA - July 21 - 26, 2014 ,\\n2014, pp. 437–440.\\n[25] S. Bubeck, V . Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz,\\nE. Kamar, P. Lee, Y . T. Lee, Y . Li, S. M. Lundberg,\\nH. Nori, H. Palangi, M. T. Ribeiro, and Y . Zhang, “Sparks\\nof artificial general intelligence: Early experiments with GPT-\\n4,”CoRR , vol. abs/2303.12712, 2023. [Online]. Available:\\nhttps://doi.org/10.48550/arXiv.2303.12712\\n[26] T. Schick, J. Dwivedi-Yu, R. Dess `ı, R. Raileanu, M. Lomeli,\\nL. Zettlemoyer, N. Cancedda, and T. Scialom, “Toolformer:\\nLanguage models can teach themselves to use tools,” CoRR ,\\nvol. abs/2302.04761, 2023. [Online]. Available: https://doi.org/\\n10.48550/arXiv.2302.04761\\n[27] S. G. Patil, T. Zhang, X. Wang, and J. E. Gonzalez,\\n“Gorilla: Large language model connected with massive\\napis,” CoRR , vol. abs/2305.15334, 2023. [Online]. Available:\\nhttps://doi.org/10.48550/arXiv.2305.15334\\n[28] L. Wang, C. Ma, X. Feng, Z. Zhang, H. Yang, J. Zhang, Z. Chen,\\nJ. Tang, X. Chen, Y . Lin, W. X. Zhao, Z. Wei, and J.-R. Wen,\\n“A survey on large language model based autonomous agents,”\\n2023.\\n[29] G. Mialon, R. Dess `ı, M. Lomeli, C. Nalmpantis, R. Pasunuru,\\nR. Raileanu, B. Rozi `ere, T. Schick, J. Dwivedi-Yu,\\nA. Celikyilmaz, E. Grave, Y . LeCun, and T. Scialom, “Augmented\\nlanguage models: a survey,” CoRR , vol. abs/2302.07842, 2023.\\n[Online]. Available: https://doi.org/10.48550/arXiv.2302.07842', metadata={'source': 'test.pdf', 'page': 10}),\n",
       " Document(page_content='[30] R.-M. Karampatsis and C. Sutton, “How often do single-\\nstatement bugs occur?” Jun. 2020. [Online]. Available: http:\\n//dx.doi.org/10.1145/3379597.3387491\\n[31] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V .\\nLe, D. Zhou et al. , “Chain-of-thought prompting elicits reason-\\ning in large language models,” Advances in neural information\\nprocessing systems , vol. 35, pp. 24 824–24 837, 2022.\\n[32] L. D. Grazia and M. Pradel, “Code search: A survey of\\ntechniques for finding code,” ACM Comput. Surv. , vol. 55,\\nno. 11, pp. 220:1–220:31, 2023. [Online]. Available: https:\\n//doi.org/10.1145/3565971\\n[33] A. Eghbali and M. Pradel, “De-hallucinator: Iterative grounding\\nfor llm-based code completion,” CoRR , vol. abs/2401.01701,\\n2024. [Online]. Available: https://doi.org/10.48550/arXiv.2401.\\n01701\\n[34] J. T. Mark Chen, “Evaluating large language models trained on\\ncode,” CoRR , vol. abs/2107.03374, 2021. [Online]. Available:\\nhttps://arxiv.org/abs/2107.03374\\n[35] J. Campos, A. Riboira, A. Perez, and R. Abreu, “Gzoltar: an\\neclipse plug-in for testing and debugging,” in Proceedings of the\\n27th IEEE/ACM international conference on automated software\\nengineering , 2012, pp. 378–381.\\n[36] A. Zeller, Why programs fail: a guide to systematic debugging .\\nElsevier, 2009.\\n[37] H. Ye, M. Martinez, X. Luo, T. Zhang, and M. Monperrus,\\n“Selfapr: Self-supervised program repair with test execution\\ndiagnostics,” in ASE 2022, Rochester, MI, USA, October\\n10-14, 2022 . ACM, 2022, pp. 92:1–92:13. [Online]. Available:\\nhttps://doi.org/10.1145/3551349.3556926\\n[38] X. D. Le, D. Lo, and C. Le Goues, “History driven program\\nrepair,” in IEEE 23rd International Conference on Software\\nAnalysis, Evolution, and Reengineering, SANER 2016, Suita,\\nOsaka, Japan, March 14-18, 2016 - Volume 1 , 2016, pp. 213–224.\\n[Online]. Available: https://doi.org/10.1109/SANER.2016.76\\n[39] Y . Ke, K. T. Stolee, C. Le Goues, and Y . Brun, “Repairing\\nprograms with semantic code search (t),” in 2015 30th IEEE/ACM\\nInternational Conference on Automated Software Engineering\\n(ASE) . IEEE, 2015, pp. 295–306.\\n[40] R. van Tonder and C. L. Goues, “Static automated program\\nrepair for heap properties,” in ICSE 2018, Gothenburg, Sweden,\\nMay 27 - June 03, 2018 , M. Chaudron, I. Crnkovic, M. Chechik,\\nand M. Harman, Eds. ACM, 2018, pp. 151–162. [Online].\\nAvailable: https://doi.org/10.1145/3180155.3180250\\n[41] Y . Liu, S. Mechtaev, P. Suboti ´c, and A. Roychoudhury, “Program\\nrepair guided by datalog-defined static analysis,” in Proceed-\\nings of the 31st ACM Joint European Software Engineering\\nConference and Symposium on the Foundations of Software\\nEngineering , 2023, pp. 1216–1228.\\n[42] N. Jain, S. Gandhi, A. Sonwane, A. Kanade, N. Natarajan,\\nS. Parthasarathy, S. Rajamani, and R. Sharma, “Staticfixer: From\\nstatic analysis to static repair,” 2023.\\n[43] D. Yang, X. Mao, L. Chen, X. Xu, Y . Lei, D. Lo, and J. He,\\n“Transplantfix: Graph differencing-based code transplantation\\nfor automated program repair,” in ASE 2022, Rochester, MI,\\nUSA, October 10-14, 2022 . ACM, 2022, pp. 107:1–107:13.\\n[Online]. Available: https://doi.org/10.1145/3551349.3556893\\n[44] A. Marginean, J. Bader, S. Chandra, M. Harman, Y . Jia, K. Mao,\\nA. Mols, and A. Scott, “Sapfix: Automated end-to-end repair at\\nscale,” in ICSE-SEIP , 2019.\\n[45] K. Wang, R. Singh, and Z. Su, “Search, align, and repair:\\ndata-driven feedback generation for introductory programming\\nexercises,” in Proceedings of the 39th ACM SIGPLAN Conference\\non Programming Language Design and Implementation, PLDI\\n2018, Philadelphia, PA, USA, June 18-22, 2018 , 2018, pp. 481–\\n495.\\n[46] R. Gupta, A. Kanade, and S. K. Shevade, “Deep reinforcement\\nlearning for syntactic error repair in student programs,” in\\nThe Thirty-Third AAAI Conference on Artificial Intelligence,\\nAAAI 2019, Honolulu, Hawaii, USA, January 27 - February 1,\\n2019 . AAAI Press, 2019, pp. 930–937. [Online]. Available:\\nhttps://doi.org/10.1609/aaai.v33i01.3301930[47] G. Sakkas, M. Endres, P. J. Guo, W. Weimer, and R. Jhala,\\n“Seq2parse: neurosymbolic parse error repair,” Proc. ACM\\nProgram. Lang. , vol. 6, no. OOPSLA2, pp. 1180–1206, 2022.\\n[Online]. Available: https://doi.org/10.1145/3563330\\n[48] T. Yu and M. Pradel, “Pinpointing and repairing performance\\nbottlenecks in concurrent programs,” Empirical Software Engi-\\nneering (EMSE) , pp. 1–38, 2017.\\n[49] J. Harer, O. Ozdemir, T. Lazovich, C. P. Reale, R. L. Russell,\\nL. Y . Kim, and S. P. Chin, “Learning to repair software vulnera-\\nbilities with generative adversarial networks,” in NeurIPS 2018,\\n3-8 December 2018, Montr ´eal, Canada. , 2018, pp. 7944–7954.\\n[50] Y . W. Chow, L. D. Grazia, and M. Pradel, “Pyty: Repairing static\\ntype errors in python,” in International Conference on Software\\nEngineering (ICSE) , 2024.\\n[51] X. Zhang, J. Zhai, S. Ma, and C. Shen, “AUTOTRAINER:\\nan automatic DNN training problem detection and repair\\nsystem,” in 43rd IEEE/ACM International Conference on\\nSoftware Engineering, ICSE 2021, Madrid, Spain, 22-30\\nMay 2021 . IEEE, 2021, pp. 359–371. [Online]. Available:\\nhttps://doi.org/10.1109/ICSE43902.2021.00043\\n[52] D. Tarlow, S. Moitra, A. Rice, Z. Chen, P. Manzagol, C. Sutton,\\nand E. Aftandilian, “Learning to fix build errors with graph2diff\\nneural networks,” in ICSE ’20 Workshops, Seoul, Republic of\\nKorea, 27 June - 19 July, 2020 . ACM, 2020, pp. 19–20.\\n[Online]. Available: https://doi.org/10.1145/3387940.3392181\\n[53] M. Vasic, A. Kanade, P. Maniatis, D. Bieber, and R. Singh,\\n“Neural program repair by jointly learning to localize and repair,”\\ninICLR , 2019.\\n[54] H. Ye, M. Martinez, and M. Monperrus, “Neural program repair\\nwith execution-based backpropagation,” in ICSE , 2022.\\n[55] Q. Zhang, C. Fang, Y . Ma, W. Sun, and Z. Chen, “A survey of\\nlearning-based automated program repair,” ACM Transactions on\\nSoftware Engineering and Methodology , vol. 33, no. 2, pp. 1–69,\\n2023.\\n[56] H. Joshi, J. P. C. S ´anchez, S. Gulwani, V . Le, G. Verbruggen,\\nand I. Radicek, “Repair is nearly generation: Multilingual\\nprogram repair with llms,” in Thirty-Seventh AAAI Conference\\non Artificial Intelligence, AAAI 2023, Washington, DC, USA,\\nFebruary 7-14, 2023 , B. Williams, Y . Chen, and J. Neville,\\nEds. AAAI Press, 2023, pp. 5131–5140. [Online]. Available:\\nhttps://doi.org/10.1609/aaai.v37i4.25642\\n[57] D. Shrivastava, H. Larochelle, and D. Tarlow, “Repository-\\nlevel prompt generation for large language models of code,” in\\nInternational Conference on Machine Learning . PMLR, 2023,\\npp. 31 693–31 715.\\n[58] C. S. Xia, M. Paltenghi, J. L. Tian, M. Pradel, and L. Zhang,\\n“Fuzz4all: Universal fuzzing with large language models,” in\\nICSE , 2024.\\n[59] C. Lemieux, J. P. Inala, S. K. Lahiri, and S. Sen, “Codamosa:\\nEscaping coverage plateaus in test generation with pre-trained\\nlarge language models,” in 45th International Conference on\\nSoftware Engineering, ser. ICSE , 2023.\\n[60] M. Sch ¨afer, S. Nadi, A. Eghbali, and F. Tip, “An empirical\\nevaluation of using large language models for automated unit\\ntest generation,” IEEE Trans. Software Eng. , vol. 50, no. 1, pp.\\n85–105, 2024. [Online]. Available: https://doi.org/10.1109/TSE.\\n2023.3334955\\n[61] G. Ryan, S. Jain, M. Shang, S. Wang, X. Ma, M. K. Ramanathan,\\nand B. Ray, “Code-aware prompting: A study of coverage guided\\ntest generation in regression setting using llm,” in FSE, 2024.\\n[62] N. Alshahwan, J. Chheda, A. Finegenova, B. Gokkaya,\\nM. Harman, I. Harper, A. Marginean, S. Sengupta, and E. Wang,\\n“Automated unit test improvement using large language models\\nat meta,” in FSE, vol. abs/2402.09171, 2024. [Online]. Available:\\nhttps://doi.org/10.48550/arXiv.2402.09171\\n[63] S. Kang, J. Yoon, and S. Yoo, “Large language models are\\nfew-shot testers: Exploring llm-based general bug reproduction,”\\nin45th IEEE/ACM International Conference on Software\\nEngineering, ICSE , 2023, pp. 2312–2323. [Online]. Available:\\nhttps://doi.org/10.1109/ICSE48619.2023.00194', metadata={'source': 'test.pdf', 'page': 11}),\n",
       " Document(page_content='[64] S. Feng and C. Chen, “Prompting is all your need: Automated\\nandroid bug replay with large language models,” in ICSE , 2024.\\n[65] R. Bairi, A. Sonwane, A. Kanade, V . D. C, A. Iyer,\\nS. Parthasarathy, S. Rajamani, B. Ashok, and S. Shet, “Codeplan:\\nRepository-level coding using llms and planning,” 2023.\\n[66] L. Gao, A. Madaan, S. Zhou, U. Alon, P. Liu, Y . Yang,\\nJ. Callan, and G. Neubig, “PAL: program-aided language\\nmodels,” CoRR , vol. abs/2211.10435, 2022. [Online]. Available:\\nhttps://doi.org/10.48550/arXiv.2211.10435', metadata={'source': 'test.pdf', 'page': 12})]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='RepairAgent: An Autonomous, LLM-Based\\nAgent for Program Repair\\nIslem Bouzenia\\nUniversity of Stuttgart\\nGermany\\nfibouzenia@esi.dzPremkumar Devanbu\\nUC Davis\\nUSA\\nptdevanbu@ucdavis.eduMichael Pradel\\nUniversity of Stuttgart\\nGermany\\nmichael@binaervarianz.de\\nAbstract —Automated program repair has emerged as\\na powerful technique to mitigate the impact of software\\nbugs on system reliability and user experience. This paper\\nintroduces RepairAgent, the first work to address the pro-\\ngram repair challenge through an autonomous agent based\\non a large language model (LLM). Unlike existing deep\\nlearning-based approaches, which prompt a model with a\\nfixed prompt or in a fixed feedback loop, our work treats\\nthe LLM as an agent capable of autonomously planning\\nand executing actions to fix bugs by invoking suitable tools.\\nRepairAgent freely interleaves gathering information about\\nthe bug, gathering repair ingredients, and validating fixes,\\nwhile deciding which tools to invoke based on the gathered', metadata={'source': 'test.pdf', 'page': 0}),\n",
       " Document(page_content='RepairAgent freely interleaves gathering information about\\nthe bug, gathering repair ingredients, and validating fixes,\\nwhile deciding which tools to invoke based on the gathered\\ninformation and feedback from previous fix attempts. Key\\ncontributions that enable RepairAgent include a set of\\ntools that are useful for program repair, a dynamically\\nupdated prompt format that allows the LLM to interact\\nwith these tools, and a finite state machine that guides the\\nagent in invoking the tools. Our evaluation on the popular\\nDefects4J dataset demonstrates RepairAgent’s effectiveness\\nin autonomously repairing 164 bugs, including 39 bugs\\nnot fixed by prior techniques. Interacting with the LLM\\nimposes an average cost of 270,000 tokens per bug, which,\\nunder the current pricing of OpenAI’s GPT-3.5 model,\\ntranslates to 14 cents per bug. To the best of our knowledge,\\nthis work is the first to present an autonomous, LLM-based\\nagent for program repair, paving the way for future agent-', metadata={'source': 'test.pdf', 'page': 0}),\n",
       " Document(page_content='translates to 14 cents per bug. To the best of our knowledge,\\nthis work is the first to present an autonomous, LLM-based\\nagent for program repair, paving the way for future agent-\\nbased techniques in software engineering.\\nI. I NTRODUCTION\\nSoftware bugs lead to system failures, security vul-\\nnerabilities, and compromised user experience. Fixing\\nbugs is a critical task in software development, but\\nif done manually, demands considerable time and ef-\\nfort. Automated program repair (APR) promises to dra-\\nmatically reduce this effort by addressing the critical\\nneed for effective and efficient bug resolution in an\\nautomated manner. Researchers and practitioners have\\nexplored various approaches to address the challenge of\\nautomatically fixing bugs [1], including techniques based\\non manually designed [2], [3] and (semi-)automatically\\nextracted [4], [5], [6] fix patterns, based on symbolic\\nconstraints [7], [8], [9], and various machine learning-', metadata={'source': 'test.pdf', 'page': 0}),\n",
       " Document(page_content='on manually designed [2], [3] and (semi-)automatically\\nextracted [4], [5], [6] fix patterns, based on symbolic\\nconstraints [7], [8], [9], and various machine learning-\\nbased approaches [10], [11], [12], [13], [14], [15], [16].The current state-of-the-art in APR predominantly\\nrevolves around large language models (LLMs). The first\\ngeneration of LLM-based repair techniques involve a\\none-time interaction with the model, where the model re-\\nceives a prompt containing the buggy code and produces\\na fixed version [17], [18]. The second and current genera-\\ntion of LLM-based repair techniques introduces iterative\\napproaches, which query the LLM repeatedly based on\\nfeedback obtained from previous fix attempts [19], [20],\\n[21].\\nA key limitation of current iterative, LLM-based repair\\ntechniques is that their hard-coded feedback loops do\\nnot allow the model to gather information about the\\nbug or existing code that may provide ingredients to\\nfix the bug. Instead, these approaches fix the context', metadata={'source': 'test.pdf', 'page': 0}),\n",
       " Document(page_content='not allow the model to gather information about the\\nbug or existing code that may provide ingredients to\\nfix the bug. Instead, these approaches fix the context\\ninformation provided in the prompt, typically to the\\nbuggy code [19], [21], and sometimes also details about\\nthe test cases that fail [20]. The feedback loop then\\nexecutes the tests on different variants of the buggy code\\nand adds any compilation errors, test failures, or other\\noutput, to the prompt of the next iteration. However,\\nthis approach fundamentally differs from the way human\\ndevelopers fix bugs, which typically involves a temporal\\ninterleaving of gathering information to understand the\\nbug, searching code that may be helpful for fixing the\\nbug, and experimenting with candidate fixes [22], [23].\\nThis paper presents RepairAgent, the first au-\\ntonomous, LLM-based agent for automated program\\nrepair. Our approach treats the LLM as an autonomous\\nagent capable of planning and executing actions to', metadata={'source': 'test.pdf', 'page': 0})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)\n",
    "documents=text_splitter.split_documents(docs)\n",
    "documents[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='RepairAgent: An Autonomous, LLM-Based\\nAgent for Program Repair\\nIslem Bouzenia\\nUniversity of Stuttgart\\nGermany\\nfibouzenia@esi.dzPremkumar Devanbu\\nUC Davis\\nUSA\\nptdevanbu@ucdavis.eduMichael Pradel\\nUniversity of Stuttgart\\nGermany\\nmichael@binaervarianz.de\\nAbstract —Automated program repair has emerged as\\na powerful technique to mitigate the impact of software\\nbugs on system reliability and user experience. This paper\\nintroduces RepairAgent, the first work to address the pro-\\ngram repair challenge through an autonomous agent based\\non a large language model (LLM). Unlike existing deep\\nlearning-based approaches, which prompt a model with a\\nfixed prompt or in a fixed feedback loop, our work treats\\nthe LLM as an agent capable of autonomously planning\\nand executing actions to fix bugs by invoking suitable tools.\\nRepairAgent freely interleaves gathering information about\\nthe bug, gathering repair ingredients, and validating fixes,\\nwhile deciding which tools to invoke based on the gathered', metadata={'source': 'test.pdf', 'page': 0}),\n",
       " Document(page_content='RepairAgent freely interleaves gathering information about\\nthe bug, gathering repair ingredients, and validating fixes,\\nwhile deciding which tools to invoke based on the gathered\\ninformation and feedback from previous fix attempts. Key\\ncontributions that enable RepairAgent include a set of\\ntools that are useful for program repair, a dynamically\\nupdated prompt format that allows the LLM to interact\\nwith these tools, and a finite state machine that guides the\\nagent in invoking the tools. Our evaluation on the popular\\nDefects4J dataset demonstrates RepairAgent’s effectiveness\\nin autonomously repairing 164 bugs, including 39 bugs\\nnot fixed by prior techniques. Interacting with the LLM\\nimposes an average cost of 270,000 tokens per bug, which,\\nunder the current pricing of OpenAI’s GPT-3.5 model,\\ntranslates to 14 cents per bug. To the best of our knowledge,\\nthis work is the first to present an autonomous, LLM-based\\nagent for program repair, paving the way for future agent-', metadata={'source': 'test.pdf', 'page': 0}),\n",
       " Document(page_content='translates to 14 cents per bug. To the best of our knowledge,\\nthis work is the first to present an autonomous, LLM-based\\nagent for program repair, paving the way for future agent-\\nbased techniques in software engineering.\\nI. I NTRODUCTION\\nSoftware bugs lead to system failures, security vul-\\nnerabilities, and compromised user experience. Fixing\\nbugs is a critical task in software development, but\\nif done manually, demands considerable time and ef-\\nfort. Automated program repair (APR) promises to dra-\\nmatically reduce this effort by addressing the critical\\nneed for effective and efficient bug resolution in an\\nautomated manner. Researchers and practitioners have\\nexplored various approaches to address the challenge of\\nautomatically fixing bugs [1], including techniques based\\non manually designed [2], [3] and (semi-)automatically\\nextracted [4], [5], [6] fix patterns, based on symbolic\\nconstraints [7], [8], [9], and various machine learning-', metadata={'source': 'test.pdf', 'page': 0}),\n",
       " Document(page_content='on manually designed [2], [3] and (semi-)automatically\\nextracted [4], [5], [6] fix patterns, based on symbolic\\nconstraints [7], [8], [9], and various machine learning-\\nbased approaches [10], [11], [12], [13], [14], [15], [16].The current state-of-the-art in APR predominantly\\nrevolves around large language models (LLMs). The first\\ngeneration of LLM-based repair techniques involve a\\none-time interaction with the model, where the model re-\\nceives a prompt containing the buggy code and produces\\na fixed version [17], [18]. The second and current genera-\\ntion of LLM-based repair techniques introduces iterative\\napproaches, which query the LLM repeatedly based on\\nfeedback obtained from previous fix attempts [19], [20],\\n[21].\\nA key limitation of current iterative, LLM-based repair\\ntechniques is that their hard-coded feedback loops do\\nnot allow the model to gather information about the\\nbug or existing code that may provide ingredients to\\nfix the bug. Instead, these approaches fix the context', metadata={'source': 'test.pdf', 'page': 0}),\n",
       " Document(page_content='not allow the model to gather information about the\\nbug or existing code that may provide ingredients to\\nfix the bug. Instead, these approaches fix the context\\ninformation provided in the prompt, typically to the\\nbuggy code [19], [21], and sometimes also details about\\nthe test cases that fail [20]. The feedback loop then\\nexecutes the tests on different variants of the buggy code\\nand adds any compilation errors, test failures, or other\\noutput, to the prompt of the next iteration. However,\\nthis approach fundamentally differs from the way human\\ndevelopers fix bugs, which typically involves a temporal\\ninterleaving of gathering information to understand the\\nbug, searching code that may be helpful for fixing the\\nbug, and experimenting with candidate fixes [22], [23].\\nThis paper presents RepairAgent, the first au-\\ntonomous, LLM-based agent for automated program\\nrepair. Our approach treats the LLM as an autonomous\\nagent capable of planning and executing actions to', metadata={'source': 'test.pdf', 'page': 0}),\n",
       " Document(page_content='This paper presents RepairAgent, the first au-\\ntonomous, LLM-based agent for automated program\\nrepair. Our approach treats the LLM as an autonomous\\nagent capable of planning and executing actions to\\nachieve the goal of fixing a bug. To this end, we equip\\nthe LLM with a set of bug repair-specific tools that\\nthe models can invoke to interact with the code base\\nin a way similar to a human developer. For example,\\nRepairAgent has tools to extract information about the\\nbug by reading specific lines of code, to gather repair\\ningredients by searching the code base, and to propose\\nand validate fixes by applying a patch and executing test\\ncases. Importantly, we do not hard-code how and when to\\nuse these tools, but instead let the LLM autonomouslyarXiv:2403.17134v1  [cs.SE]  25 Mar 2024', metadata={'source': 'test.pdf', 'page': 0}),\n",
       " Document(page_content='decide which tool to invoke next, based on previously\\ngathered information and feedback from previous fix\\nattempts.\\nOur approach is enabled by three key components.\\nFirst, a general-purpose LLM, such as GPT-3.5, which\\nwe query repeatedly with a dynamically updated prompt.\\nWe contribute a novel prompt format that guides the\\nLLM through the bug repair process, and that gets\\nupdated based on the commands invoked by the LLM\\nand the results of the previous command executions.\\nSecond, a set of tools that the LLM can invoke to interact\\nwith the code base. We present a set of 14 tools designed\\nto cover different steps a human developer would take\\nwhen fixing a bug, such as reading specific lines of code,\\nsearching the code base, and applying a patch. Third, a\\nmiddleware that orchestrates the communication between\\nthe LLM and the tools. We present novel techniques for\\nguiding tool invocations through a finite state machine\\nand for heuristically interpreting possibly incorrect LLM', metadata={'source': 'test.pdf', 'page': 1}),\n",
       " Document(page_content='the LLM and the tools. We present novel techniques for\\nguiding tool invocations through a finite state machine\\nand for heuristically interpreting possibly incorrect LLM\\noutputs. The iterative loop of RepairAgent continues\\nuntil the agent declares to have found a suitable fix, or\\nuntil exhausting a budget of iterations.\\nTo evaluate the effectiveness of our approach, we\\napply it to all 835 bugs in the Defects4J [24] dataset,\\na widely used benchmark for evaluating program repair\\ntechniques. RepairAgent successfully fixes 164 bugs,\\nincluding 74 and 90 bugs of Defects4J v1.2 and v2.0,\\nrespectively. The correctly fixed bugs include 49 bugs\\nthat require fixing more than one line, showing that\\nRepairAgent is capable of fixing complex bugs. Com-\\npared to state-of-the-art techniques [19], [21], Repair-\\nAgent successfully fixes 39 bugs not fixed by prior work.\\nMeasuring the costs imposed by interacting with the\\nLLM, we find that RepairAgent imposes an average cost', metadata={'source': 'test.pdf', 'page': 1}),\n",
       " Document(page_content='Agent successfully fixes 39 bugs not fixed by prior work.\\nMeasuring the costs imposed by interacting with the\\nLLM, we find that RepairAgent imposes an average cost\\nof 270,000 tokens per bug, which, under the current\\npricing of OpenAI’s GPT-3.5 model, translates to 14\\ncents per bug. Overall, our results show that our agent-\\nbased approach establishes a new state of the art in\\nprogram repair.\\nIn summary, this paper contributes the following:\\n•An autonomous, LLM-based agent for program repair.\\n•A dynamically updated prompt format that guides the\\nLLM through the bug repair process.\\n•A set of tools that enable a LLM to to perform steps\\na human developer would take when fixing a bug.\\n•A middleware that orchestrates the communication\\nbetween the LLM and the tools.\\n•Empirical evidence that RepairAgent establishes a\\nnew state of the art by successfully fixing 164 bugs,\\nincluding 39 bugs not fixed by prior work.\\n•We will release the implementation of RepairAgent as', metadata={'source': 'test.pdf', 'page': 1}),\n",
       " Document(page_content='new state of the art by successfully fixing 164 bugs,\\nincluding 39 bugs not fixed by prior work.\\n•We will release the implementation of RepairAgent as\\nopen-source to foster future work.\\nTo the best of our knowledge, there currently is no\\npublished work on an autonomous, LLM-based agentfor any code-generation task. We envision RepairAgent\\nto pave the way for future agent-based techniques in\\nsoftware engineering.\\nII. B ACKGROUND ON LLM-B ASED , AUTONOMOUS\\nAGENTS\\nBy virtue of being trained on vast amounts of web\\nknowledge, including natural language and source code,\\nLLMs have demonstrated remarkable abilities in achiev-\\ning human-level performance for various tasks [25]. A\\npromising way of using these abilities are LLM-based\\nagents that autonomously plan and execute actions to\\nachieve a goal. The basic idea is to query the LLM with\\na prompt that contains the current state of the world, the\\ngoal to be achieved, and a set of actions that could be', metadata={'source': 'test.pdf', 'page': 1}),\n",
       " Document(page_content='achieve a goal. The basic idea is to query the LLM with\\na prompt that contains the current state of the world, the\\ngoal to be achieved, and a set of actions that could be\\nperformed next. The model than decides which action to\\nperform, and the feedback from performing the action\\nis integrated into the next prompt. One way to represent\\n“actions” is through tools that the model can invoke to\\ninteract with the world [26], [27]. Recent surveys provide\\na comprehensive overview of LLM-based, autonomous\\nagents [28] and of LLM agents equipped with tools\\ninvoked via APIs [29]. The potential of such agents\\nfor software engineering currently is not well explored,\\nwhich this paper aims to address for the challenging task\\nof automated program repair.\\nIII. A PPROACH\\nA. Overview\\nFigure 1 gives an overview of the RepairAgent ap-\\nproach, which consists of three components: an LLM\\nagent, a set of tools, and a middleware that orchestrates\\nthe communication between the two. Given a bug to fix,', metadata={'source': 'test.pdf', 'page': 1}),\n",
       " Document(page_content='proach, which consists of three components: an LLM\\nagent, a set of tools, and a middleware that orchestrates\\nthe communication between the two. Given a bug to fix,\\nthe middleware initializes the LLM agent with a prompt\\nthat contains task information and instructions on how\\nto perform it by using the provided tools. The LLM\\nresponds by suggesting a call to one of the available\\ntools, which the middleware parses and then executes.\\nThe output of the tool is then integrated into the prompt\\nfor the next invocation of the LLM, and the process\\ncontinues iteratively until the bug is fixed or a predefined\\nbudget is exhausted.\\nB. Terminology\\nRepairAgent proceeds in multiple iterations, which we\\ncall cycles:\\nDefinition 1 (Cycle) .Acycle represents one round of\\ninteraction with the LLM agent, which consists of the\\nfollowing steps:\\n1) Query the agent\\n2) Post-process the response\\n3) Execute the command suggested by the agent\\n4) Update the dynamic prompt based on the command’s\\noutput', metadata={'source': 'test.pdf', 'page': 1}),\n",
       " Document(page_content='LLM agent Tools Middleware\\n User\\nRead code\\nSearch code base\\nRun tests\\nState and discard\\nhypotheses\\nWrite a patchInvoke command\\nCommand to\\nexecute nextQuery agent with\\ndynamic prompt\\nRaw tool outputGuide agent via a\\nstate machine\\nParse and refine\\nLLM output\\nStore and summarize\\ntool outputRepairAgent\\nFixBugFig. 1: Overview of RepairAgent.\\nTABLE I: Sections of the dynamically updated prompt.\\nPrompt section Nature\\nRole Static\\nGoals Static\\nGuidelines Static\\nState description Dynamic\\nAvailable tools Dynamic\\nGathered information Dynamic\\nSpecification of output format Static\\nLast executed command and result Dynamic\\nIn each cycle, the approach queries the LLM once.\\nThe input to the model is updated based on commands\\n(calls to tools) invoked by the LLM, and their results,\\nin previous cycles. We call the model input a dynamic\\nprompt:\\nDefinition 2 (Dynamic prompt) .The dynamic prompt\\nis a sequence of text sections P= [s0, s1, ..., s n], where', metadata={'source': 'test.pdf', 'page': 2}),\n",
       " Document(page_content='in previous cycles. We call the model input a dynamic\\nprompt:\\nDefinition 2 (Dynamic prompt) .The dynamic prompt\\nis a sequence of text sections P= [s0, s1, ..., s n], where\\neach section siis one of the following (where si(c)refers\\nto a section during a cycle c):\\n•Astatic section , which remains the same across all\\ncycles, i.e., si(c) =si(c′)for all c, c′.\\n•Adynamic section , which may differ across cycles,\\ni.e., there may exist c, c′withsi(c) ̸=si(c′).\\nC. Dynamic Prompting of the Repair Agent\\nThe repair agent is an LLM trained on natural lan-\\nguage and source code, such as GPT-3.5. RepairAgent\\nqueries the LLM with a dynamic prompt that consists\\nof a sequence of static and dynamic sections, as listed\\nin Table I. We describe each section in detail in the\\nfollowing.\\n1) Role: This section of the prompt defines the\\nagent’s area of expertise, which is to resolve bugs in\\nJava code, and outlines the agent’s primary objective:\\nunderstanding and fixing bugs. The prompt emphasizes', metadata={'source': 'test.pdf', 'page': 2}),\n",
       " Document(page_content='agent’s area of expertise, which is to resolve bugs in\\nJava code, and outlines the agent’s primary objective:\\nunderstanding and fixing bugs. The prompt emphasizes\\nthat the agent’s decision-making process is autonomous\\nand should not rely on user assistance.\\n2) Goals: We define five goals for the agent to pursue,\\nwhich remain the same across all cycles:\\n•Locate the bug: Execute tests and use fault localiza-\\ntion techniques to pinpoint the bug’s location. Skip\\nthis goal when fault localization information is already\\nprovided in the prompt.•Gather information about the bug: Analyze the lines\\nof code associated with the bug to understand the bug.\\n•Suggest simple fixes to the bug: Start by suggesting\\nsimple fixes.\\n•Suggest complex fixes: If simple fixes prove ineffec-\\ntive, explore and propose more complex ones.\\n•Iterate over the previous goals: Continue to gather\\ninformation and to suggest fixes until finding a fix.\\n3) Guidelines: We provide a set of guidelines. First,', metadata={'source': 'test.pdf', 'page': 2}),\n",
       " Document(page_content='•Iterate over the previous goals: Continue to gather\\ninformation and to suggest fixes until finding a fix.\\n3) Guidelines: We provide a set of guidelines. First,\\nwe inform the model that there are diverse kinds of\\nbugs, ranging from single-line issues to multi-line bugs\\nthat may entail changing, removing, or adding lines.\\nBased on the observation that many bugs can be fixed by\\nrelatively simple, recurring fix patterns [30], we provide\\na list of recurring fix patterns. The list is based on\\nthe patterns described in prior work on single-statement\\nbugs in Java [30]. For each pattern, we provide a short\\nnatural language description and an example of buggy\\nand fixed code. Second, we instruct the model to insert\\ncomments above the modified code, which serves two\\npurposes. On the one hand, the comments allow the\\nmodel to explain its reasoning, which has been shown\\nto enhance the reasoning abilities of LLMs [31]. On\\nthe other hand, commenting will ultimately help human', metadata={'source': 'test.pdf', 'page': 2}),\n",
       " Document(page_content='model to explain its reasoning, which has been shown\\nto enhance the reasoning abilities of LLMs [31]. On\\nthe other hand, commenting will ultimately help human\\ndevelopers in understanding the nature of the edits.\\nThird, we instruct the model to conclude its reasoning\\nwith a clearly defined next step that can be translated into\\na call to a tool. Finally, we describe that there is a limited\\nbudget of tool invocations, highlighting the importance\\nof efficiency in selecting the next steps. Specifically, we\\nspecify a maximum number of cycles (40 by default).\\n4) State Description: To guide the LLM agent toward\\nusing the available tools in an effective and meaningful\\nway, we define a finite state machine that constrains\\nwhich tools are available at a given point in time.\\nThe motivation is that we observed the LLM agent\\nto frequently get lost in aimless exploration in earlier\\nexperiments without such guidance. Figure 2 shows the\\nfinite state machine, which we design to mimic the states', metadata={'source': 'test.pdf', 'page': 2}),\n",
       " Document(page_content='to frequently get lost in aimless exploration in earlier\\nexperiments without such guidance. Figure 2 shows the\\nfinite state machine, which we design to mimic the states\\na human developer would go through when fixing a bug.\\nEach state is associated with a set of tools available to the\\nagent, which are described in Section III-D. Importantly,\\nthe agent is free to transition between states at any\\npoint in time by using tools. That is, despite providing', metadata={'source': 'test.pdf', 'page': 2}),\n",
       " Document(page_content='Try to fix\\nthe bugCollect\\ninformation\\nto fix the\\nbugUnderstand\\nthe bugrun_tests\\nextract_tests\\nrun_fault_localization\\nexpress_hypothesissearch_code_base\\nfind_similar_api_calls\\ngenerate_method_body\\nread_range\\nget_classes_and_methods\\nextract_method\\nDone\\nwrite_fix\\nread_rangediscard_hypothesisdiscard_hypothesis\\nwrite_fixcollect_more_information\\ngoal_accomplishedFig. 2: State machine to guide selection of tools.\\nguidance, the state machine does not enforce a strict\\norder of tool invocations.\\nThe state description section of the prompt informs\\nthe agent about its current state:\\n•Understand the bug : The agent starts in this state,\\nwhere it can collect information related to the failing\\ntest cases and the bug’s location. Once the agent\\nhas an understanding of the bug, it formulates a\\nhypothesis to describe the nature of the bug and the\\nreason behind it. Throughout the repair process, the\\nagent may refute earlier hypotheses and express new\\nones. After expressing a hypothesis, the agent will', metadata={'source': 'test.pdf', 'page': 3}),\n",
       " Document(page_content='reason behind it. Throughout the repair process, the\\nagent may refute earlier hypotheses and express new\\nones. After expressing a hypothesis, the agent will\\nautomatically switch to the next state.\\n•Collect information to fix the bug : In this state the\\nagent collects information that help suggest a fix for\\nthe bug expressed by the hypothesis, e.g., by searching\\nfor specific repair ingredients or by reading possibly\\nrelevant code. Once the agent has gathered enough\\ninformation to attempt a fix, it can transition to the\\nnext state.\\n•Try to fix the bug : In this state, the agent tries to\\nfix the bug based on its current hypothesis and the\\ncollected information. Each fix attempt modifies the\\ncode base and is validated by executing the test cases.\\nIf necessarily, the agent can go back to one of the\\nprevious states to establish a new hypothesis or to\\ngather additional information.\\nIn addition to the three above states, RepairAgent has a', metadata={'source': 'test.pdf', 'page': 3}),\n",
       " Document(page_content='If necessarily, the agent can go back to one of the\\nprevious states to establish a new hypothesis or to\\ngather additional information.\\nIn addition to the three above states, RepairAgent has a\\nfinal state, “Done” , which the agent can reach by calling\\na specific command that indicates the success of repair.\\n5) Available Tools: This section of the prompt de-\\nscribes a set of tools that the agent can call at the current\\nstate. Each tool has a name, a description, and a set ofinterface Response {\\nthoughts :string;\\ncommand :{\\nname :string;\\nargs :Record<string, any>;\\n};\\n}\\nFig. 3: JSON format of the response of the model.\\n{\\n\"thoughts\" :\"With the hypothesis in mind,\\nit’s time to gather more information to\\nformulate potential fixes for the bug. I\\nshould focus on understanding the context\\naround the condition\\n’if (x < 0 && prev == ’-’)’\\nto come up with effective solutions.\",\\n\"command\" :{\\n\"name\" :\"search_code_base\",\\n\"args\" :{\\n\"key_words\" :\\n[\"addNumber\",\\n\"CodeConsumer\",', metadata={'source': 'test.pdf', 'page': 3}),\n",
       " Document(page_content='around the condition\\n’if (x < 0 && prev == ’-’)’\\nto come up with effective solutions.\",\\n\"command\" :{\\n\"name\" :\"search_code_base\",\\n\"args\" :{\\n\"key_words\" :\\n[\"addNumber\",\\n\"CodeConsumer\",\\n\"if (x < 0 && prev == ’-’)\" ]\\n}}}\\nFig. 4: Example of a response of the repair agent.\\ntyped arguments (Section III-D).\\n6) Gathered Information: A key ability of the repair\\nagent is to gather information about the bug and the code\\nbase, which serves as the basis for deciding which com-\\nmands to invoke next. To make this information available\\nto the agent, we maintain a prompt section that lists the\\ninformation gathered by the different tool invocations.\\nIntuitively, this section of the prompt serves as a memory\\nfor the agent, allowing it to recall information from pre-\\nvious cycles. The gathered information is structured into\\ndifferent subsections, where each subsection contains the\\noutputs produced by a specific tool.\\n7) Specification of Output Format: Given the dy-', metadata={'source': 'test.pdf', 'page': 3}),\n",
       " Document(page_content='different subsections, where each subsection contains the\\noutputs produced by a specific tool.\\n7) Specification of Output Format: Given the dy-\\nnamic prompt, the LLM agent provides one response\\nper cycle. To enable the middleware to parse the re-\\nsponse, we specify the expected output format (Figure 3).\\nThe “thoughts” field provides a textual description of\\nthe agent’s reasoning when deciding about the next\\ncommand. Asking the agent to express its thoughts\\nincreases the transparency and interpretability of the\\napproach, provides a way to debug potential issues in the\\nagent’s decision-making process, and helps improve the\\nreasoning abilities of LLMs [31]. The “command” field\\nspecifies the next command to be executed, consisting of\\nthe name of the tool to invoke and the set of arguments.\\nFor example, Figure 4 shows a response of the LLM\\nagent. The model expresses the need to collect more\\ninformation to understand the bug. It then suggests a', metadata={'source': 'test.pdf', 'page': 3}),\n",
       " Document(page_content='For example, Figure 4 shows a response of the LLM\\nagent. The model expresses the need to collect more\\ninformation to understand the bug. It then suggests a\\ncommand that searches the code base with a list of\\nkeywords.', metadata={'source': 'test.pdf', 'page': 3}),\n",
       " Document(page_content='8) Last Executed Command and Result: This section\\nof the prompt contains the last command (tool name and\\narguments) that was executed (if any) and the output it\\nproduced. The rationale is to remind the agent of the\\nlast step it took, and to make it aware of any problems\\nthat occurred during the execution of the command.\\nFurthermore, we remind the agent how many cycles have\\nalready been executed, and how many cycles are left\\nD. Tools for the Agent to Use\\nA key novelty in our approach is to let an LLM agent\\nautonomously decide which tools to invoke to fix a bug.\\nThe tools we provide to the agent (Table II) are inspired\\nby the tools that developers use in their IDEs.\\n1) Reading and Extracting Code: A prerequisite for\\nfixing a bug is to read and understand relevant parts\\nof the code base. Instead of hard-coding the context\\nprovided to the LLM [19], [20], [21], we let the agent\\ndecide which parts of the code to read, based on four\\ntools. The read range tool allows the agent to extract', metadata={'source': 'test.pdf', 'page': 4}),\n",
       " Document(page_content='provided to the LLM [19], [20], [21], we let the agent\\ndecide which parts of the code to read, based on four\\ntools. The read range tool allows the agent to extract\\na range of lines from a specific file, which is useful\\nto obtain a focused view of a particular section of\\ncode. To obtain an overview of the code structure,\\nthegetclasses and methods tool retrieves all class and\\nmethod names within a given file. By invoking the\\nextract method tool, the agent can retrieve the imple-\\nmentation(s) of methods that match a given method name\\nwithin a given file. Finally, we offer the extract tests\\ntool, which extracts the code of test cases that resulted in\\nfailure. The tool is crucial to understand details of failing\\ntests, such as input values and the expected output.\\n2) Search and generate code: Motivated by the fact\\nthat human developers commonly search for code [32],\\nwe present tools that allow the agent to search for\\nspecific code snippets. These tools are useful for the', metadata={'source': 'test.pdf', 'page': 4}),\n",
       " Document(page_content='that human developers commonly search for code [32],\\nwe present tools that allow the agent to search for\\nspecific code snippets. These tools are useful for the\\nagent to better understand the context of a bug and\\nto gather repair ingredients, i.e., code fragments that\\ncould become part of a fix. The search code base tool\\nenables the agent to locate instances of particular key-\\nwords within the entire code base. For example, the\\nagent can use this tool to find occurrences of vari-\\nables, methods, and classes. Given a set of keywords,\\nthe tool performs an approximate matching against all\\nsource code files in the project. Specifically, the tool\\nsplits each keyword into subtokens based on camel\\ncase, underscores, and periods, and then searches for\\neach subtoken in the code. For example, searching\\nforquickSortArray yields matches for sortArray ,\\nquickSort ,arrayQuickSort , and other related vari-\\nations. The output of the tool is a nested dictionary,', metadata={'source': 'test.pdf', 'page': 4}),\n",
       " Document(page_content='forquickSortArray yields matches for sortArray ,\\nquickSort ,arrayQuickSort , and other related vari-\\nations. The output of the tool is a nested dictionary,\\norganized by file names, classes, and method names, that\\nprovides the keywords that match a method’s content.\\nAnother search tool, find similar apicalls , allows the\\nagent to identify and extract usages of a method, whichis useful to fix incorrect method calls. Without such a\\ntool, LLMs tend to hallucinate method calls that do not\\nexist in the code base [33]. Given a code snippet that\\ncontains a method call, the tool extracts the name of the\\ncalled method, and then searches for calls to methods\\nwith the same name. The agent can restrict the search to\\na specific file or search the entire code base.\\nIn addition to searching for existing code, Repair-\\nAgent offers a tool that generates new code by invoking\\nanother LLM. The tool is inspired by the success of\\nLLM-based code completion tools, such as Copilot [34],', metadata={'source': 'test.pdf', 'page': 4}),\n",
       " Document(page_content='Agent offers a tool that generates new code by invoking\\nanother LLM. The tool is inspired by the success of\\nLLM-based code completion tools, such as Copilot [34],\\nwhich human developers increasingly use when fixing\\nbugs. Given the code preceding a method and the signa-\\nture of the method, the generate method body tool asks\\nan LLM to generate the body of the method. The query to\\nthe code-generating LLM is independent of the dynamic\\nprompt used by the overall RepairAgent approach, and\\nmay use a different model. In our evaluation, we use the\\nsame LLM for both the repair agent and as the code-\\ngenerating LLM of this tool. The tool limits the given\\ncode context to 12k tokens and sets a limit of 4k tokens\\nfor the generated code.\\n3) Testing and Patching: The next category of tools\\nis related to running tests and applying patches. The\\nrun tests tool allows the agent to execute the test suite\\nof the project. It produces a report that indicates whether', metadata={'source': 'test.pdf', 'page': 4}),\n",
       " Document(page_content='is related to running tests and applying patches. The\\nrun tests tool allows the agent to execute the test suite\\nof the project. It produces a report that indicates whether\\nthe tests passed or failed. In case of test failures, the tool\\ncleans the output of the test runner, e.g., by removing\\nentries of the stack trace that are outside of the current\\nproject. The rationale is that LLMs have a limited prompt\\nsize and that irrelevant information may confuse the\\nmodel. The run fault localization tool retrieves fault\\nlocalization information, which is useful to understand\\nwhich parts of the code are likely to contain the bug.\\nRepairAgent offers two variants of this tool: Either, it\\nprovides perfect fault localization information, i.e., the\\nfile(s) and line(s) that need to be edited to fix the bug,\\nor it invokes an existing fault localization tool, such\\nas GZoltar [35], to calculate fault localization scores.\\nAs common in the field of program repair, we assume', metadata={'source': 'test.pdf', 'page': 4}),\n",
       " Document(page_content='or it invokes an existing fault localization tool, such\\nas GZoltar [35], to calculate fault localization scores.\\nAs common in the field of program repair, we assume\\nperfect fault localization as the default.\\nOnce the agent has gathered sufficient information to\\nfix the bug, it can apply a patch to the code base using the\\nwrite fixtool. RepairAgent aims at repairing arbitrarily\\ncomplex bugs, including multi-line and even multi-file\\nbugs. The write fixtool expects a patch in a specific\\nJSON format, which indicates the insertions, deletions,\\nand modifications to be made in each file. Figure 5 shows\\nan example of a patch in this format. Given a patch,\\nthe tool applies the changes to the code base and runs\\nthe test suite. If the tests fail, the write fixreverts the\\nchanges, giving the agent a clean code base to try another\\nfix. Motivated by the observation that some fix attempts\\nare almost correct, the write fixtool requests the LLM', metadata={'source': 'test.pdf', 'page': 4}),\n",
       " Document(page_content='TABLE II: Repair-related tools invoked by RepairAgent.\\nTool Description\\nRead and extract code:\\nread range Read a range of lines in a file.\\ngetclasses and methods Get the names of all classes and methods in a file.\\nextract method Given a method name, extract method implementations from a file.\\nextract tests Given the failure report from JUnit or ANT, extract the code of failing test cases.\\nSearch and generate code:\\nsearch code base Scans all Java files within a project for a list of keywords.\\nfind similar api calls Given a code snippet that calls an API, search for similar API calls in the project.\\ngenerate method body Ask an LLM (GPT3.5 by default) to generate the body of a method based on code proceeding the method.\\nTesting and patching:\\nrun tests Run the test suite of a project.\\nrun fault localization Retrieve pre-existing localization information or run a fault localization tool.', metadata={'source': 'test.pdf', 'page': 5}),\n",
       " Document(page_content='Testing and patching:\\nrun tests Run the test suite of a project.\\nrun fault localization Retrieve pre-existing localization information or run a fault localization tool.\\nwrite fix Apply a patch to the code base and execute the test suite of the project. Changes are reverted automatically if tests\\nfail. Moves the agent into the ’Try to fix the bug’ state.\\nControl:\\nexpress hypothesis Express a hypothesis about the bug. Moves the agent into the ’Collect information to fix the bug’ state.\\ncollect more information Move the agent back to the ’Collect information to fix the bug’ state.\\ndiscard hypothesis Discard the current hypothesis about the bug and move back to the ’Understand the bug’ state.\\ngoal accomplished Declare that the goal has been accomplished and exiting the repair process.\\n[\\n{\\n\"file_path\" :\"jfree/data/time/Week.java\",\\n\"insertions\" : [\\n{\\n\"line_number\" :175,\\n\"new_lines\" : [\\n\"// ...new lines to insert...\\\\n\",\\n\"// ...more new lines...\\\\n\" ]\\n}\\n],\\n\"deletions\" : [179, 183 ],', metadata={'source': 'test.pdf', 'page': 5}),\n",
       " Document(page_content='[\\n{\\n\"file_path\" :\"jfree/data/time/Week.java\",\\n\"insertions\" : [\\n{\\n\"line_number\" :175,\\n\"new_lines\" : [\\n\"// ...new lines to insert...\\\\n\",\\n\"// ...more new lines...\\\\n\" ]\\n}\\n],\\n\"deletions\" : [179, 183 ],\\n\"modifications\" : [\\n{\\n\"line_number\" :179,\\n\"modified_line\" :\" if (dataset == null\\n) {\\\\n\"\\n}\\n]\\n},\\n{\\n\"file_path\" :\"org/jfree/data/time/Day.java\"\\n,\\n\"insertions\" : [] ,\\n\"deletions\" : [307],\\n\"modifications\" : []\\n}\\n]\\nFig. 5: Example of patch given to the write fixtool.\\nto sample multiple variants of the suggested fix. By\\ndefault, RepairAgent samples 30 variants at max. Given\\nthe generated variants, the approach removes duplicates\\nand launch tests for every variant.\\n4) Control: The final set of tools do not directly cor-\\nrespond to a tool a human developer may use, but rather\\nallow the agent to move between states (Figure 2). The\\nexpress hypothesis tool empowers the agent to articulatea hypothesis regarding the nature of the bug and to\\ntransition to the ’Collect information to fix the bug’ state.', metadata={'source': 'test.pdf', 'page': 5}),\n",
       " Document(page_content='express hypothesis tool empowers the agent to articulatea hypothesis regarding the nature of the bug and to\\ntransition to the ’Collect information to fix the bug’ state.\\nInversely, the discard hypothesis tool allows the agent\\nto discard a hypothesis that is no longer viable, which\\nleads back to the ’Understand the bug’ state. Together,\\nthe two commands enforce a structured approach to\\nhypothesis formulation, aligning with work on scientific\\ndebugging [36], [20]. In case the agent has tried multiple\\nfixes without success, the collect more information tool\\nallows the agent to revert to the ’Collect information\\nto fix the bug’ state. Finally, once the agent has found\\nat least one fix that passes all tests, it can invoke the\\ngoal accomplished tool, which terminates RepairAgent.\\nE. Middleware\\nThe middleware component plays a crucial role in\\nRepairAgent, orchestrating the communication between\\nthe LLM agent and the tools. It performs the steps in\\nDefinition 1 as described in the following.', metadata={'source': 'test.pdf', 'page': 5}),\n",
       " Document(page_content='The middleware component plays a crucial role in\\nRepairAgent, orchestrating the communication between\\nthe LLM agent and the tools. It performs the steps in\\nDefinition 1 as described in the following.\\n1) Parsing and Refining LLM Output: At the begin-\\nning of each cycle, the middleware queries the LLM\\nwith the current prompt. Ideally, the response adheres\\nperfectly to the expected format (Figure 3). In practice,\\nhowever, the LLM may produce responses that deviate\\nfrom the expected format, e.g., due to hallucinations or\\nsyntactic errors. For example, the LLM may provide\\na “path” argument while the tool expects a “file path”\\nargument.\\nRepairAgent tries to heuristically rectify such issues\\nby mapping the output to the expected format in three\\nsteps. First, it tries to map the tool mentioned in the\\nresponse to one of the available tools. Specifically, the\\napproach checks if the predicted tool name npredicted is', metadata={'source': 'test.pdf', 'page': 5}),\n",
       " Document(page_content='a substring of the name of any available tool nactual , or\\nvice versa, and if yes, considers nactual to be the desired\\ntool. In case the above matching fails, the approach\\nchecks if the Levenshtein distance between npredicted\\nand any nactual is below a threshold (0.1 by default).\\nSecond, the approach tries to map the argument names\\nprovided in the response to the expected arguments of the\\ntool, following the same logic as above. Third, the ap-\\nproach handles invalid argument values by heuristically\\nmapping or replacing them, e.g., by replacing a predicted\\nfile path with a valid one. If the heuristic mapping\\nfails or produces multiple possible tool invocations, the\\nmiddleware informs the LLM about the issue via the\\n“Last executed command and result” prompt section and\\nenters a new cycle.\\nIn addition to rectifying minor mistakes in the re-\\nsponse, the middleware also checks for repeated invo-\\ncations of the same tool with the same arguments. If', metadata={'source': 'test.pdf', 'page': 6}),\n",
       " Document(page_content='enters a new cycle.\\nIn addition to rectifying minor mistakes in the re-\\nsponse, the middleware also checks for repeated invo-\\ncations of the same tool with the same arguments. If\\nthe agent suggests the exact same command as in a\\nprevious cycle, which would yield the same results, the\\nmiddleware informs the agent about the repetition and\\nenters a new cycle.\\n2) Calling the Tool: Once the middleware has re-\\nceived a valid command from the LLM, it calls the\\ncorresponding tool. To prevent tool executions to inter-\\nfere with the host environment or RepairAgent itself,\\nthe middleware executes the command in an isolated\\nenvironment that contains a cloned version of the buggy\\nrepository.\\n3) Updating the Prompt: Given the output of the\\ntool, the middleware updates all dynamic sections of the\\nprompt for the next cycle. In particular, the middleware\\nupdates the state description and the available tools,\\nappends the tool’s output to the gathered information,', metadata={'source': 'test.pdf', 'page': 6}),\n",
       " Document(page_content='prompt for the next cycle. In particular, the middleware\\nupdates the state description and the available tools,\\nappends the tool’s output to the gathered information,\\nand replaces the section that shows the last executed\\ncommand.\\nIV. I MPLEMENTATION\\nWe use Python 3.10 as our primary programming\\nlanguage. Docker is used to containerize and isolate\\ncommand executions for enhanced reliability and repro-\\nducibility. RepairAgent is built on top of the AutoGPT\\nframework and GPT-3.5-0125 from OpenAI. To parse\\nand interact with Java code, we use ANTLR.\\nV. E VALUATION\\nTo evaluate our approach we aim to answer the\\nfollowing research questions:\\nRQ1 How effective is RepairAgent at fixing real-world\\nbugs?\\nRQ2 What are the costs of the approach?\\nRQ3 How does the LLM agent use the available tools?TABLE III: Results on Defects4J.\\nProject Bugs Plausible Correct ChatRepair ITER SelfAPR\\nChart 26 14 11 15 10 7\\nCli 39 9 8 5 6 8\\nClosure 174 27 27 37 18 20\\nCodec 18 10 9 8 3 8\\nCollections 4 1 1 0 0 1', metadata={'source': 'test.pdf', 'page': 6}),\n",
       " Document(page_content='Project Bugs Plausible Correct ChatRepair ITER SelfAPR\\nChart 26 14 11 15 10 7\\nCli 39 9 8 5 6 8\\nClosure 174 27 27 37 18 20\\nCodec 18 10 9 8 3 8\\nCollections 4 1 1 0 0 1\\nCompress 47 10 10 2 4 7\\nCsv 16 6 6 3 2 1\\nGson 18 3 3 3 0 1\\nJacksonCore 26 5 5 3 3 3\\nJacksondatabind 112 18 11 9 0 8\\nJacksonXml 6 1 1 1 0 1\\nJsoup 93 18 18 14 0 6\\nJxPath 22 0 0 0 0 1\\nLang 63 17 17 21 0 10\\nMath 106 29 29 32 0 22\\nMockito 38 6 6 6 0 3\\nTime 26 3 2 3 2 3\\nDefects4Jv1.2 395 96 74 114 57 64\\nDefects4Jv2 440 90 90 48 — 46\\nTotal 835 186 164 162 57 110\\nA. Experimental Setup\\na) Dataset: We apply RepairAgent to bugs in the\\nDefects4J dataset [24]. We use the entire Defects4J\\ndataset, which consists of 835 real-world bugs from\\n17 Java projects, including 395 bugs from 6 projects\\nin Defects4Jv1.2, as well as another 440 bugs and 11\\nprojects added in Defects4Jv2. Evaluating on the entire\\ndataset allows us to assess the generalization capabilities\\nof RepairAgent to different projects and bugs, without', metadata={'source': 'test.pdf', 'page': 6}),\n",
       " Document(page_content='projects added in Defects4Jv2. Evaluating on the entire\\ndataset allows us to assess the generalization capabilities\\nof RepairAgent to different projects and bugs, without\\nrestricting the evaluation, e.g., based on the number of\\nlines, hunks, or files that need to be fixed.\\nb) Baselines: We compare with three existing re-\\npair techniques: ChatRepair [19], ITER [21], and Self-\\nAPR [37]. ChatRepair and ITER are two very recent ap-\\nproaches and have been shown to be the current state of\\nthe art. All three baseline approaches follow an iterative\\napproach that incorporates feedback from previous patch\\nattempts. Unlike RepairAgent, the baselines do not use\\nan autonomous, LLM-based agent. We compare against\\nthe baselines based on patches provided by the authors\\nof the respective approaches.\\nc) Metrics of success: Similar to past work, we\\nreport both the number of plausible and correct patches.\\nA fix is plausible if it passes all test cases, but is not', metadata={'source': 'test.pdf', 'page': 6}),\n",
       " Document(page_content='of the respective approaches.\\nc) Metrics of success: Similar to past work, we\\nreport both the number of plausible and correct patches.\\nA fix is plausible if it passes all test cases, but is not\\nnecessarily correct. To determine whether a fix is correct,\\nwe automatically check whether it syntactically matches\\nthe developer-created fix. If this is not the case, we\\nmanually determine whether the RepairAgent-generated\\nfix is semantically consistent with the developer-created\\nfix. If and only if either of the two checks succeeds, we\\nconsider the fix to be correct .\\nB. RQ1: Effectiveness\\n1) Overall Results: Table III summarizes the ef-\\nfectiveness of RepairAgent in fixing the 835 bugs in', metadata={'source': 'test.pdf', 'page': 6}),\n",
       " Document(page_content='TABLE IV: Distribution of fixes by location type\\nBug type RepairAgent ChatRepair ITER SelfAPR\\nSingle-line 115 133 36 83\\nMulti-line* 46 29 14 24\\nMulti-file 3 0 4 3\\nFig. 6: Intersection of the set fixes with related work.\\nDefects4J. The approach generates plausible fixes for\\n186 bugs. While not necessarily correct, plausible fixes\\npass all test cases and may still provide developers a\\nhint about what should be changed. RepairAgent gener-\\nates correct fixes for 164 bugs, which are semantically\\nconsistent with the developer-provided patches. Being\\nable to fix bugs from different projects shows that the\\napproach can generalize to code bases of multiple do-\\nmains. Furthermore, RepairAgent creates fixes for bugs\\nof different levels of complexity. Specifically, as shown\\nin Table IV, the approach fixes 115 single-line bugs, 46\\nmulti-line (single-file) bugs, and 3 multi-file bugs.\\n2) Comparison with Prior Work: The right-hand side\\nof Table III compares RepairAgent with the baseline', metadata={'source': 'test.pdf', 'page': 7}),\n",
       " Document(page_content='multi-line (single-file) bugs, and 3 multi-file bugs.\\n2) Comparison with Prior Work: The right-hand side\\nof Table III compares RepairAgent with the baseline\\napproaches ChatRepair, ITER, and SelfAPR. Previous\\nto this work, ChatRepair had established a new state\\nof the art in APR by fixing 162 bugs in Defects4J.\\nRepairAgent achieves a comparable record by fixing\\na total of 164 bugs. Our work particularly excels in\\nDefects4Jv2, where RepairAgent fixes 90 bugs, while\\nChatRepair only fixes 48 bugs. To further compare the\\nsets of fixed bugs, Figure 6 shows the overlaps between\\ndifferent approaches. As often observed in the field of\\nAPR, different approaches complement each other to\\nsome extent. In particular, RepairAgent fixes 39 bugs that\\nwere not fixed by any of the three baselines. Comparing\\nthe complexity of the bug fixes, as shown on the right-\\nhand side of Table IV, RepairAgent is particularly more\\neffective, compared to other tools, for bugs that require', metadata={'source': 'test.pdf', 'page': 7}),\n",
       " Document(page_content='the complexity of the bug fixes, as shown on the right-\\nhand side of Table IV, RepairAgent is particularly more\\neffective, compared to other tools, for bugs that require\\nmore than a single-line fix. We attribute this result to the\\nRepairAgent’s ability to autonomously retrieve suitable\\nrepair ingredients and the fact that the agent can perform\\nedits to an arbitrary number of lines and files.if (cfa != null) {\\nfor (Node finallyNode :\\ncfa.finallyMap.get(parent)) {\\n- cfa.createEdge(fromNode, Branch.UNCOND,\\nfinallyNode);\\n+ cfa.createEdge(fromNode, Branch.ON_EX,\\nfinallyNode);}}\\nFig. 7: Closure-14, bug fixed by RepairAgent.\\nSeparator sep = (Separator)\\nelementPairs.get(0);\\n+ if (sep.iAfterParser == null &&\\nsep.iAfterPrinter == null) {\\nPeriodFormatter f =\\ntoFormatter(elementPairs.subList(2,\\nsize), notPrinter, notParser);\\nsep = sep.finish(f.getPrinter(),\\nf.getParser());\\nreturn new PeriodFormatter(sep, sep);\\n+ }\\nFig. 8: Time-27, bug fixed by RepairAgent.', metadata={'source': 'test.pdf', 'page': 7}),\n",
       " Document(page_content='size), notPrinter, notParser);\\nsep = sep.finish(f.getPrinter(),\\nf.getParser());\\nreturn new PeriodFormatter(sep, sep);\\n+ }\\nFig. 8: Time-27, bug fixed by RepairAgent.\\n3) Examples: Figures 7 and 8 showcase interesting\\nbugs fixed exclusively by RepairAgent. In the\\nexample of Figure 7, the agent used the tool\\nfind similar apicalls to search for calls similar\\ntocfa.createEdge(fromNode, Branch.UNCOND,\\nfinallyNode); which returns a similar call that is\\nfound in another file but passes Branch.ON_EX to the\\nmethod call instead of Branch.UNCOND . The latter was\\nused as the repair ingredient by the agent.\\nIn the second example, RepairAgent benefeted from\\nthe tool generate method body to generate the miss-\\ning if-statement which led to suggesting a correct fix\\nafterwards.\\nFrom one side, these examples illustrate the clever\\nand proper usage of available tools by the repair agent.\\nFrom the other side, it shows how useful these tools at\\nfinding repair ingredients that traditional approaches and', metadata={'source': 'test.pdf', 'page': 7}),\n",
       " Document(page_content='and proper usage of available tools by the repair agent.\\nFrom the other side, it shows how useful these tools at\\nfinding repair ingredients that traditional approaches and\\nprevious work failed to find.\\nC. RQ2: Costs of the Approach\\nWe measure three kinds of costs imposed by Repair-\\nAgent: (i) Time taken to fix a bug. (ii) The number of\\ntokens consumed by queries to the LLM, which is rel-\\nevant both for commercial models, such as the GPT-3.5\\nused here, and for self-hosted models, where the number\\nof tokens determines the computational costs. (iii) The\\nmonetary costs associated with the token consumption,\\nbased on OpenAI’s pricing as of March 2024.\\nOur findings are summarized in Figure 9. The median\\ntime taken to address a bug is 920 seconds, with minimal\\nvariation between fixed (870 seconds) and unfixed bugs.\\nSurprisingly, fixed bugs do not consistently exhibit lower\\nrepair times. This is due to RepairAgent’s autonomous', metadata={'source': 'test.pdf', 'page': 7}),\n",
       " Document(page_content='(a) Time.\\n (b) Tokens/money consumption.\\nFig. 9: Distribution of cost metrics per bug (time, number of token, and monetary costs).\\nnature, where the repair process continues until the\\ngoal accomplished command is invoked or the cycles\\nbudget is exhausted. The figure shows several outliers\\nwhere bug fixing attempt takes multiple hours. Repair-\\nAgent spends 99% of the total time in tool executions,\\nmostly running tests. This cost could be reduced in the\\nfuture by executing selected test cases only.\\nAnalyzing the costs imposed by the LLM, we find a\\nmedian consumption of approximately 270,000 tokens,\\nequating to around 14 cents (US dollars). The number\\nof tokens consumed by fixed bugs (21,000) is clearly\\nlower than by unfixed bugs (315,000). This difference\\nis because the agent continues to extract additional\\ninformation for not yet fixed bugs, saturating the prompt\\nwith operations, such as reading more lines of code or\\nperforming extensive searches.\\nD. RQ3: Usage of Tools by the Agent', metadata={'source': 'test.pdf', 'page': 8}),\n",
       " Document(page_content='information for not yet fixed bugs, saturating the prompt\\nwith operations, such as reading more lines of code or\\nperforming extensive searches.\\nD. RQ3: Usage of Tools by the Agent\\nTo better understand the approach, we evaluate how\\nthe agent uses the available tools.\\n1) Adherence to Output Format: A notable challenge\\nin designing an LLM-based agent is the inherent infor-\\nmality and natural language noise present in the model’s\\noutput. We categorize the output of the model as follows:\\n•Fully parsable output: Responses that adhere to the\\nJSON format without requiring further refinement\\n(87.7%).\\n•Unparsable output: Responses that do not conform to\\nthe JSON format (2.3% of the responses).\\n•Partially parsable output: Responses in JSON format\\nbut with missing or misnamed fields requiring refine-\\nment by the middleware (9.9%).\\nA parsed output does not guarantee a correctly specified\\ncommand, which the middleware tries to rectify. At this', metadata={'source': 'test.pdf', 'page': 8}),\n",
       " Document(page_content='but with missing or misnamed fields requiring refine-\\nment by the middleware (9.9%).\\nA parsed output does not guarantee a correctly specified\\ncommand, which the middleware tries to rectify. At this\\nphase, the output may fall into the following categories:•Correct command name: The name of the command\\nis correct (97.9%).\\n•Non-existent command: The command could not be\\nfound or mapped to an existing one (1.4%).\\n•Mapped command: The command does not exist but\\ncan be mapped to an existing command by the mid-\\ndleware (0.7%).\\n•Correct arguments: The arguments are correct\\n(90.1%).\\n•Unrefinable arguments list: The command exists or\\nwas mapped, but the list of arguments is incomplete\\nor has incorrect names (1.9%).\\n•Refinable arguments list: The middleware successfully\\nmaps the list of arguments into a correct one (8.0%).\\nOverall, these results show that our heuristic refinement\\nof LLM outputs contributes to the effectiveness and\\nrobustness of RepairAgent.', metadata={'source': 'test.pdf', 'page': 8}),\n",
       " Document(page_content='maps the list of arguments into a correct one (8.0%).\\nOverall, these results show that our heuristic refinement\\nof LLM outputs contributes to the effectiveness and\\nrobustness of RepairAgent.\\n2) Frequency of Tools Invocation: On average,\\nRepairAgent makes 35 calls per bug, which also cor-\\nresponds to the number of cycles. Figure 10 shows the\\nfrequency of tool invocations, where we distinguish be-\\ntween fixed (i.e., “correct”) and unfixed (i.e., “plausible”\\nonly or completely unfixed) bugs. The LLM agent uses\\nthe full range of tools, with the most frequently called\\ntool being write fix(average of 6 calls for fixed bugs\\nand 17 calls for unfixed bugs. Around 7% of write fix\\ninvocations in unfixed bugs produce plausible patches,\\ncompared to 44% in fixed bugs.\\nVI. T HREATS TO VALIDITY AND LIMITATIONS\\nWhile RepairAgent shows promising results when\\nrepairing Defects4J bugs, we acknowledge several poten-\\ntial threats to validity and inherent limitations: (i) Data', metadata={'source': 'test.pdf', 'page': 8}),\n",
       " Document(page_content='While RepairAgent shows promising results when\\nrepairing Defects4J bugs, we acknowledge several poten-\\ntial threats to validity and inherent limitations: (i) Data\\nleakage: As we evaluate on GPT-3.5 and its training data\\nis not publicly known, the LLM may have seen parts of', metadata={'source': 'test.pdf', 'page': 8}),\n",
       " Document(page_content='Fig. 10: Frequency of tool invocations (average per bug).\\nthe Java projects during training. While we acknowl-\\nedge this risk, our approach does not solely depend on\\nknowing about a bug, but rather the ability to collect\\ninformation to fix the bug. We also note that the closest\\ncompetitor, ChatRepair, also uses GPT-3.5, and thus\\nfaces the same risk. (ii) Missing test cases: Defects4J\\nhas at least one failing test case for each bug, which\\nmay not be the case for real-world usage scenarios. It\\nwill be interesting to evaluate RepairAgent on bugs with\\nno a-priori available error-revealing test cases in future\\nwork. (iii) Fault localization: Inaccurate or imprecise\\nfault localization could lead to suboptimal repair sug-\\ngestions or incorrect diagnoses. (iv) Non-deterministic\\noutput of LLMs: The inherently non-deterministic nature\\nof LLMs may result in different outcomes between two\\nconsecutive runs of RepairAgent. The large number of\\nbugs we evaluate on mitigates this risk. Moreover, we', metadata={'source': 'test.pdf', 'page': 9}),\n",
       " Document(page_content='of LLMs may result in different outcomes between two\\nconsecutive runs of RepairAgent. The large number of\\nbugs we evaluate on mitigates this risk. Moreover, we\\nmake logs of all interactions with the LLM available to\\nensure reproducibility.\\nVII. R ELATED WORK\\na) Automated program repair: Automated program\\nrepair [1] has received significant attention. Some ap-\\nproaches address it as a search problem based on\\nmanually designed code mutation rules and fix pat-\\nterns [2], [38], [3]. Alternatively, transformation rules\\ncan be derived (semi-)automatically from human-written\\npatches [4], [5], [6]. Other approaches use symbolic\\nconstraints to derive fixes [7], [39], [8], [9], integrate\\nrepair into a static analysis that identifies bugs [40], [41],\\n[42], or replace buggy code with similar code from the\\nsame project [43]. APR has been successfully deployed\\nin industrial contexts [5], [44]. Beyond functional bugs,several techniques target other kinds of problems, such', metadata={'source': 'test.pdf', 'page': 9}),\n",
       " Document(page_content='same project [43]. APR has been successfully deployed\\nin industrial contexts [5], [44]. Beyond functional bugs,several techniques target other kinds of problems, such\\nas syntax errors [45], [46], [47], performance bugs [48],\\nvulnerabilities [49], type errors [50], common issues in\\ndeep learning code [51], and build errors [52].\\nb) Learning-based program repair: While early\\nwork uses machine learning to rank and select candidate\\nfixes [10], more recent work uses machine learning to\\ngenerate fixes. Approaches include neural machine trans-\\nlation models that map buggy code into fixed code [11],\\n[12], [13], [14], models that predict tree transforma-\\ntions [15], [16], neural architectures for specific kinds of\\nbugs [53], and repair-specific training regimes [54], [37].\\nWe refer to a recent survey for a more comprehensive\\ndiscussion [55]. Unlike the above work, RepairAgent and\\nthe work discussed below use a general-purpose LLM,\\ninstead of training a task-specific model.', metadata={'source': 'test.pdf', 'page': 9}),\n",
       " Document(page_content='discussion [55]. Unlike the above work, RepairAgent and\\nthe work discussed below use a general-purpose LLM,\\ninstead of training a task-specific model.\\nc) LLM-based program repair: LLMs have moti-\\nvated researchers to apply them to program repair, e.g.,\\nin studies that explore prompts [18], [17] and in a tech-\\nnique that prompts the model with error messages [56].\\nThese approaches perform a one-time interaction with\\nthe model, where the model receives a prompt with code\\nand produces a fix. The most recent repair techniques\\nintroduce iterative approaches, which query the LLM\\nrepeatedly based on feedback obtained from previous\\nfix attempts [19], [20], [21]. RepairAgent also queries\\nthe model multiple times, but fundamentally differs by\\npursuing an agent-based approach. Section V empirically\\ncompares RepairAgent to the most closely related itera-\\ntive approaches [19], [21].\\nd) LLMs for code generation and code editing:\\nBeyond program repair, LLMs have been applied to', metadata={'source': 'test.pdf', 'page': 9}),\n",
       " Document(page_content='compares RepairAgent to the most closely related itera-\\ntive approaches [19], [21].\\nd) LLMs for code generation and code editing:\\nBeyond program repair, LLMs have been applied to\\na variety of other code generation and code editing\\ntasks, including code completion [34], [57], fuzzing [58],\\ngenerating and improving unit tests [59], [60], [61], [62],\\n[63], [64], multi-step code editing [65]. Unlike our work,\\nnone of these approaches uses an agent-based approach.\\ne) LLM-based agents: The idea to let LLM agents\\nautonomously plan and perform complex tasks is rel-\\natively new and has been applied to tasks outside of\\nsoftware engineering [28]. To the best of our knowledge,\\nour work is the first to apply an LLM-based agent to\\nprogram repair or any other code generation problem in\\nsoftware engineering. RepairAgent is inspired by prior\\nwork [29] on augmenting LLMs with tools invoked via\\nAPIs [26], [27] and with the ability to generate and\\nexecute code [66]. Our key contribution in applying these', metadata={'source': 'test.pdf', 'page': 9}),\n",
       " Document(page_content='work [29] on augmenting LLMs with tools invoked via\\nAPIs [26], [27] and with the ability to generate and\\nexecute code [66]. Our key contribution in applying these\\nideas to a software engineering task is to define tools that\\nare useful for program repair and a prompt format that\\nallows the LLM to interact with these tools.\\nVIII. C ONCLUSION\\nThis paper presents a pioneering technique for bug\\nrepair based on an autonomous agent powered by Large', metadata={'source': 'test.pdf', 'page': 9}),\n",
       " Document(page_content='Language Models (LLMs). Through extensive experi-\\nmentation, we validate the effectiveness and potential\\nof our approach. Further exploration and refinement of\\nautonomous agent-based techniques will help generalize\\nto more difficult and diverse types of bugs if equipped\\nwith the right tools.\\nREFERENCES\\n[1] C. Le Goues, M. Pradel, and A. Roychoudhury, “Automated\\nprogram repair,” Commun. ACM , vol. 62, no. 12, pp. 56–65,\\n2019. [Online]. Available: https://doi.org/10.1145/3318162\\n[2] C. Le Goues, T. Nguyen, S. Forrest, and W. Weimer, “Genprog:\\nA generic method for automatic software repair,” IEEE Trans.\\nSoftware Eng. , vol. 38, no. 1, pp. 54–72, 2012.\\n[3] K. Liu, A. Koyuncu, D. Kim, and T. F. Bissyand ´e,\\n“Tbar: revisiting template-based automated program repair,”\\ninProceedings of the 28th ACM SIGSOFT International\\nSymposium on Software Testing and Analysis, ISSTA 2019,\\nBeijing, China, July 15-19, 2019 , D. Zhang and A. Møller,\\nEds. ACM, 2019, pp. 31–42. [Online]. Available: https:', metadata={'source': 'test.pdf', 'page': 10}),\n",
       " Document(page_content='Symposium on Software Testing and Analysis, ISSTA 2019,\\nBeijing, China, July 15-19, 2019 , D. Zhang and A. Møller,\\nEds. ACM, 2019, pp. 31–42. [Online]. Available: https:\\n//doi.org/10.1145/3293882.3330577\\n[4] D. Kim, J. Nam, J. Song, and S. Kim, “Automatic patch gen-\\neration learned from human-written patches.” in International\\nConference on Software Engineering (ICSE) , 2013, pp. 802–811.\\n[5] J. Bader, A. Scott, M. Pradel, and S. Chandra, “Getafix:\\nLearning to fix bugs automatically,” Proc. ACM Program.\\nLang. , vol. 3, no. OOPSLA, pp. 159:1–159:27, 2019. [Online].\\nAvailable: https://doi.org/10.1145/3360585\\n[6] R. Bavishi, H. Yoshida, and M. R. Prasad, “Phoenix:\\nautomated data-driven synthesis of repairs for static analysis\\nviolations,” in ESEC/SIGSOFT FSE 2019, Tallinn, Estonia,\\nAugust 26-30, 2019 , M. Dumas, D. Pfahl, S. Apel, and\\nA. Russo, Eds. ACM, 2019, pp. 613–624. [Online]. Available:\\nhttps://doi.org/10.1145/3338906.3338952', metadata={'source': 'test.pdf', 'page': 10}),\n",
       " Document(page_content='August 26-30, 2019 , M. Dumas, D. Pfahl, S. Apel, and\\nA. Russo, Eds. ACM, 2019, pp. 613–624. [Online]. Available:\\nhttps://doi.org/10.1145/3338906.3338952\\n[7] H. D. T. Nguyen, D. Qi, A. Roychoudhury, and S. Chandra,\\n“Semfix: program repair via semantic analysis,” in 35th Inter-\\nnational Conference on Software Engineering, ICSE ’13, San\\nFrancisco, CA, USA, May 18-26, 2013 , 2013, pp. 772–781.\\n[8] J. Xuan, M. Martinez, F. Demarco, M. Clement, S. L. Marcote,\\nT. Durieux, D. Le Berre, and M. Monperrus, “Nopol: Automatic\\nrepair of conditional statement bugs in java programs,” IEEE\\nTransactions on Software Engineering , vol. 43, no. 1, pp. 34–55,\\n2016.\\n[9] S. Mechtaev, J. Yi, and A. Roychoudhury, “Angelix: Scalable\\nmultiline program patch synthesis via symbolic analysis,” in\\nProceedings of the 38th international conference on software\\nengineering , 2016, pp. 691–701.\\n[10] F. Long and M. Rinard, “Automatic patch generation by learning', metadata={'source': 'test.pdf', 'page': 10}),\n",
       " Document(page_content='Proceedings of the 38th international conference on software\\nengineering , 2016, pp. 691–701.\\n[10] F. Long and M. Rinard, “Automatic patch generation by learning\\ncorrect code,” in Proceedings of the 43rd Annual ACM SIGPLAN-\\nSIGACT Symposium on Principles of Programming Languages,\\nPOPL 2016, St. Petersburg, FL, USA, January 20 - 22, 2016 ,\\n2016, pp. 298–312.\\n[11] R. Gupta, S. Pal, A. Kanade, and S. K. Shevade, “Deepfix:\\nFixing common C language errors by deep learning,” in\\nProceedings of the Thirty-First AAAI Conference on Artificial\\nIntelligence , 2017, pp. 1345–1351. [Online]. Available: http:\\n//aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14603\\n[12] M. Tufano, J. Pantiuchina, C. Watson, G. Bavota, and\\nD. Poshyvanyk, “On learning meaningful code changes via\\nneural machine translation,” in Proceedings of the 41st\\nInternational Conference on Software Engineering, ICSE 2019,\\nMontreal, QC, Canada, May 25-31, 2019 , 2019, pp. 25–36.', metadata={'source': 'test.pdf', 'page': 10}),\n",
       " Document(page_content='neural machine translation,” in Proceedings of the 41st\\nInternational Conference on Software Engineering, ICSE 2019,\\nMontreal, QC, Canada, May 25-31, 2019 , 2019, pp. 25–36.\\n[Online]. Available: https://dl.acm.org/citation.cfm?id=3339509\\n[13] T. Lutellier, H. V . Pham, L. Pang, Y . Li, M. Wei, and L. Tan,\\n“Coconut: combining context-aware neural translation models\\nusing ensemble for program repair,” in ISSTA ’20: 29th ACM\\nSIGSOFT Virtual Event, USA, July 18-22, 2020 , S. Khurshid\\nand C. S. Pasareanu, Eds. ACM, 2020, pp. 101–114. [Online].\\nAvailable: https://doi.org/10.1145/3395363.3397369[14] Z. Chen, S. Kommrusch, M. Tufano, L. Pouchet, D. Poshyvanyk,\\nand M. Monperrus, “SequenceR: Sequence-to-sequence learning\\nfor end-to-end program repair,” IEEE Trans. Software Eng. ,\\nvol. 47, no. 9, pp. 1943–1959, 2021. [Online]. Available:\\nhttps://doi.org/10.1109/TSE.2019.2940179\\n[15] Y . Li, S. Wang, and T. N. Nguyen, “Dlfix: Context-based code', metadata={'source': 'test.pdf', 'page': 10}),\n",
       " Document(page_content='vol. 47, no. 9, pp. 1943–1959, 2021. [Online]. Available:\\nhttps://doi.org/10.1109/TSE.2019.2940179\\n[15] Y . Li, S. Wang, and T. N. Nguyen, “Dlfix: Context-based code\\ntransformation learning for automated program repair,” in ICSE ,\\n2020.\\n[16] Q. Zhu, Z. Sun, Y . Xiao, W. Zhang, K. Yuan, Y . Xiong,\\nand L. Zhang, “A syntax-guided edit decoder for neural\\nprogram repair,” in ESEC/FSE ’21 Athens, Greece, August\\n23-28, 2021 , D. Spinellis, G. Gousios, M. Chechik, and M. D.\\nPenta, Eds. ACM, 2021, pp. 341–353. [Online]. Available:\\nhttps://doi.org/10.1145/3468264.3468544\\n[17] C. S. Xia, Y . Wei, and L. Zhang, “Automated program repair in\\nthe era of large pre-trained language models,” in 2023 IEEE/ACM\\n45th International Conference on Software Engineering (ICSE) ,\\n2023, pp. 1482–1494.\\n[18] N. Jiang, K. Liu, T. Lutellier, and L. Tan, “Impact of\\ncode language models on automated program repair,” in 45th\\nIEEE/ACM International Conference on Software Engineering,', metadata={'source': 'test.pdf', 'page': 10}),\n",
       " Document(page_content='2023, pp. 1482–1494.\\n[18] N. Jiang, K. Liu, T. Lutellier, and L. Tan, “Impact of\\ncode language models on automated program repair,” in 45th\\nIEEE/ACM International Conference on Software Engineering,\\nICSE . IEEE, 2023, pp. 1430–1442. [Online]. Available:\\nhttps://doi.org/10.1109/ICSE48619.2023.00125\\n[19] C. S. Xia and L. Zhang, “Keep the conversation going: Fixing\\n162 out of 337 bugs for $0.42 each using ChatGPT,” 2023.\\n[20] S. Kang, B. Chen, S. Yoo, and J. Lou, “Explainable automated\\ndebugging via large language model-driven scientific debugging,”\\nCoRR , vol. abs/2304.02195, 2023. [Online]. Available: https:\\n//doi.org/10.48550/arXiv.2304.02195\\n[21] H. Ye and M. Monperrus, “Iter: Iterative neural repair for multi-\\nlocation patches,” in ICSE , 2024.\\n[22] A. J. Ko, B. A. Myers, M. J. Coblenz, and H. H. Aung, “An\\nexploratory study of how developers seek, relate, and collect\\nrelevant information during software maintenance tasks,” IEEE', metadata={'source': 'test.pdf', 'page': 10}),\n",
       " Document(page_content='[22] A. J. Ko, B. A. Myers, M. J. Coblenz, and H. H. Aung, “An\\nexploratory study of how developers seek, relate, and collect\\nrelevant information during software maintenance tasks,” IEEE\\nTransactions on software engineering , vol. 32, no. 12, pp. 971–\\n987, 2006.\\n[23] M. B ¨ohme, E. O. Soremekun, S. Chattopadhyay, E. Ugherughe,\\nand A. Zeller, “Where is the bug and how is it fixed? an\\nexperiment with practitioners,” in Proceedings of the 2017 11th\\njoint meeting on foundations of software engineering , 2017, pp.\\n117–128.\\n[24] R. Just, D. Jalali, and M. D. Ernst, “Defects4j: a database\\nof existing faults to enable controlled testing studies for java\\nprograms,” in International Symposium on Software Testing and\\nAnalysis, ISSTA ’14, San Jose, CA, USA - July 21 - 26, 2014 ,\\n2014, pp. 437–440.\\n[25] S. Bubeck, V . Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz,\\nE. Kamar, P. Lee, Y . T. Lee, Y . Li, S. M. Lundberg,\\nH. Nori, H. Palangi, M. T. Ribeiro, and Y . Zhang, “Sparks', metadata={'source': 'test.pdf', 'page': 10}),\n",
       " Document(page_content='[25] S. Bubeck, V . Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz,\\nE. Kamar, P. Lee, Y . T. Lee, Y . Li, S. M. Lundberg,\\nH. Nori, H. Palangi, M. T. Ribeiro, and Y . Zhang, “Sparks\\nof artificial general intelligence: Early experiments with GPT-\\n4,”CoRR , vol. abs/2303.12712, 2023. [Online]. Available:\\nhttps://doi.org/10.48550/arXiv.2303.12712\\n[26] T. Schick, J. Dwivedi-Yu, R. Dess `ı, R. Raileanu, M. Lomeli,\\nL. Zettlemoyer, N. Cancedda, and T. Scialom, “Toolformer:\\nLanguage models can teach themselves to use tools,” CoRR ,\\nvol. abs/2302.04761, 2023. [Online]. Available: https://doi.org/\\n10.48550/arXiv.2302.04761\\n[27] S. G. Patil, T. Zhang, X. Wang, and J. E. Gonzalez,\\n“Gorilla: Large language model connected with massive\\napis,” CoRR , vol. abs/2305.15334, 2023. [Online]. Available:\\nhttps://doi.org/10.48550/arXiv.2305.15334\\n[28] L. Wang, C. Ma, X. Feng, Z. Zhang, H. Yang, J. Zhang, Z. Chen,\\nJ. Tang, X. Chen, Y . Lin, W. X. Zhao, Z. Wei, and J.-R. Wen,', metadata={'source': 'test.pdf', 'page': 10}),\n",
       " Document(page_content='https://doi.org/10.48550/arXiv.2305.15334\\n[28] L. Wang, C. Ma, X. Feng, Z. Zhang, H. Yang, J. Zhang, Z. Chen,\\nJ. Tang, X. Chen, Y . Lin, W. X. Zhao, Z. Wei, and J.-R. Wen,\\n“A survey on large language model based autonomous agents,”\\n2023.\\n[29] G. Mialon, R. Dess `ı, M. Lomeli, C. Nalmpantis, R. Pasunuru,\\nR. Raileanu, B. Rozi `ere, T. Schick, J. Dwivedi-Yu,\\nA. Celikyilmaz, E. Grave, Y . LeCun, and T. Scialom, “Augmented\\nlanguage models: a survey,” CoRR , vol. abs/2302.07842, 2023.\\n[Online]. Available: https://doi.org/10.48550/arXiv.2302.07842', metadata={'source': 'test.pdf', 'page': 10}),\n",
       " Document(page_content='[30] R.-M. Karampatsis and C. Sutton, “How often do single-\\nstatement bugs occur?” Jun. 2020. [Online]. Available: http:\\n//dx.doi.org/10.1145/3379597.3387491\\n[31] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V .\\nLe, D. Zhou et al. , “Chain-of-thought prompting elicits reason-\\ning in large language models,” Advances in neural information\\nprocessing systems , vol. 35, pp. 24 824–24 837, 2022.\\n[32] L. D. Grazia and M. Pradel, “Code search: A survey of\\ntechniques for finding code,” ACM Comput. Surv. , vol. 55,\\nno. 11, pp. 220:1–220:31, 2023. [Online]. Available: https:\\n//doi.org/10.1145/3565971\\n[33] A. Eghbali and M. Pradel, “De-hallucinator: Iterative grounding\\nfor llm-based code completion,” CoRR , vol. abs/2401.01701,\\n2024. [Online]. Available: https://doi.org/10.48550/arXiv.2401.\\n01701\\n[34] J. T. Mark Chen, “Evaluating large language models trained on\\ncode,” CoRR , vol. abs/2107.03374, 2021. [Online]. Available:\\nhttps://arxiv.org/abs/2107.03374', metadata={'source': 'test.pdf', 'page': 11}),\n",
       " Document(page_content='01701\\n[34] J. T. Mark Chen, “Evaluating large language models trained on\\ncode,” CoRR , vol. abs/2107.03374, 2021. [Online]. Available:\\nhttps://arxiv.org/abs/2107.03374\\n[35] J. Campos, A. Riboira, A. Perez, and R. Abreu, “Gzoltar: an\\neclipse plug-in for testing and debugging,” in Proceedings of the\\n27th IEEE/ACM international conference on automated software\\nengineering , 2012, pp. 378–381.\\n[36] A. Zeller, Why programs fail: a guide to systematic debugging .\\nElsevier, 2009.\\n[37] H. Ye, M. Martinez, X. Luo, T. Zhang, and M. Monperrus,\\n“Selfapr: Self-supervised program repair with test execution\\ndiagnostics,” in ASE 2022, Rochester, MI, USA, October\\n10-14, 2022 . ACM, 2022, pp. 92:1–92:13. [Online]. Available:\\nhttps://doi.org/10.1145/3551349.3556926\\n[38] X. D. Le, D. Lo, and C. Le Goues, “History driven program\\nrepair,” in IEEE 23rd International Conference on Software\\nAnalysis, Evolution, and Reengineering, SANER 2016, Suita,', metadata={'source': 'test.pdf', 'page': 11}),\n",
       " Document(page_content='[38] X. D. Le, D. Lo, and C. Le Goues, “History driven program\\nrepair,” in IEEE 23rd International Conference on Software\\nAnalysis, Evolution, and Reengineering, SANER 2016, Suita,\\nOsaka, Japan, March 14-18, 2016 - Volume 1 , 2016, pp. 213–224.\\n[Online]. Available: https://doi.org/10.1109/SANER.2016.76\\n[39] Y . Ke, K. T. Stolee, C. Le Goues, and Y . Brun, “Repairing\\nprograms with semantic code search (t),” in 2015 30th IEEE/ACM\\nInternational Conference on Automated Software Engineering\\n(ASE) . IEEE, 2015, pp. 295–306.\\n[40] R. van Tonder and C. L. Goues, “Static automated program\\nrepair for heap properties,” in ICSE 2018, Gothenburg, Sweden,\\nMay 27 - June 03, 2018 , M. Chaudron, I. Crnkovic, M. Chechik,\\nand M. Harman, Eds. ACM, 2018, pp. 151–162. [Online].\\nAvailable: https://doi.org/10.1145/3180155.3180250\\n[41] Y . Liu, S. Mechtaev, P. Suboti ´c, and A. Roychoudhury, “Program\\nrepair guided by datalog-defined static analysis,” in Proceed-', metadata={'source': 'test.pdf', 'page': 11}),\n",
       " Document(page_content='Available: https://doi.org/10.1145/3180155.3180250\\n[41] Y . Liu, S. Mechtaev, P. Suboti ´c, and A. Roychoudhury, “Program\\nrepair guided by datalog-defined static analysis,” in Proceed-\\nings of the 31st ACM Joint European Software Engineering\\nConference and Symposium on the Foundations of Software\\nEngineering , 2023, pp. 1216–1228.\\n[42] N. Jain, S. Gandhi, A. Sonwane, A. Kanade, N. Natarajan,\\nS. Parthasarathy, S. Rajamani, and R. Sharma, “Staticfixer: From\\nstatic analysis to static repair,” 2023.\\n[43] D. Yang, X. Mao, L. Chen, X. Xu, Y . Lei, D. Lo, and J. He,\\n“Transplantfix: Graph differencing-based code transplantation\\nfor automated program repair,” in ASE 2022, Rochester, MI,\\nUSA, October 10-14, 2022 . ACM, 2022, pp. 107:1–107:13.\\n[Online]. Available: https://doi.org/10.1145/3551349.3556893\\n[44] A. Marginean, J. Bader, S. Chandra, M. Harman, Y . Jia, K. Mao,\\nA. Mols, and A. Scott, “Sapfix: Automated end-to-end repair at\\nscale,” in ICSE-SEIP , 2019.', metadata={'source': 'test.pdf', 'page': 11}),\n",
       " Document(page_content='[44] A. Marginean, J. Bader, S. Chandra, M. Harman, Y . Jia, K. Mao,\\nA. Mols, and A. Scott, “Sapfix: Automated end-to-end repair at\\nscale,” in ICSE-SEIP , 2019.\\n[45] K. Wang, R. Singh, and Z. Su, “Search, align, and repair:\\ndata-driven feedback generation for introductory programming\\nexercises,” in Proceedings of the 39th ACM SIGPLAN Conference\\non Programming Language Design and Implementation, PLDI\\n2018, Philadelphia, PA, USA, June 18-22, 2018 , 2018, pp. 481–\\n495.\\n[46] R. Gupta, A. Kanade, and S. K. Shevade, “Deep reinforcement\\nlearning for syntactic error repair in student programs,” in\\nThe Thirty-Third AAAI Conference on Artificial Intelligence,\\nAAAI 2019, Honolulu, Hawaii, USA, January 27 - February 1,\\n2019 . AAAI Press, 2019, pp. 930–937. [Online]. Available:\\nhttps://doi.org/10.1609/aaai.v33i01.3301930[47] G. Sakkas, M. Endres, P. J. Guo, W. Weimer, and R. Jhala,\\n“Seq2parse: neurosymbolic parse error repair,” Proc. ACM\\nProgram. Lang. , vol. 6, no. OOPSLA2, pp. 1180–1206, 2022.', metadata={'source': 'test.pdf', 'page': 11}),\n",
       " Document(page_content='“Seq2parse: neurosymbolic parse error repair,” Proc. ACM\\nProgram. Lang. , vol. 6, no. OOPSLA2, pp. 1180–1206, 2022.\\n[Online]. Available: https://doi.org/10.1145/3563330\\n[48] T. Yu and M. Pradel, “Pinpointing and repairing performance\\nbottlenecks in concurrent programs,” Empirical Software Engi-\\nneering (EMSE) , pp. 1–38, 2017.\\n[49] J. Harer, O. Ozdemir, T. Lazovich, C. P. Reale, R. L. Russell,\\nL. Y . Kim, and S. P. Chin, “Learning to repair software vulnera-\\nbilities with generative adversarial networks,” in NeurIPS 2018,\\n3-8 December 2018, Montr ´eal, Canada. , 2018, pp. 7944–7954.\\n[50] Y . W. Chow, L. D. Grazia, and M. Pradel, “Pyty: Repairing static\\ntype errors in python,” in International Conference on Software\\nEngineering (ICSE) , 2024.\\n[51] X. Zhang, J. Zhai, S. Ma, and C. Shen, “AUTOTRAINER:\\nan automatic DNN training problem detection and repair\\nsystem,” in 43rd IEEE/ACM International Conference on\\nSoftware Engineering, ICSE 2021, Madrid, Spain, 22-30', metadata={'source': 'test.pdf', 'page': 11}),\n",
       " Document(page_content='an automatic DNN training problem detection and repair\\nsystem,” in 43rd IEEE/ACM International Conference on\\nSoftware Engineering, ICSE 2021, Madrid, Spain, 22-30\\nMay 2021 . IEEE, 2021, pp. 359–371. [Online]. Available:\\nhttps://doi.org/10.1109/ICSE43902.2021.00043\\n[52] D. Tarlow, S. Moitra, A. Rice, Z. Chen, P. Manzagol, C. Sutton,\\nand E. Aftandilian, “Learning to fix build errors with graph2diff\\nneural networks,” in ICSE ’20 Workshops, Seoul, Republic of\\nKorea, 27 June - 19 July, 2020 . ACM, 2020, pp. 19–20.\\n[Online]. Available: https://doi.org/10.1145/3387940.3392181\\n[53] M. Vasic, A. Kanade, P. Maniatis, D. Bieber, and R. Singh,\\n“Neural program repair by jointly learning to localize and repair,”\\ninICLR , 2019.\\n[54] H. Ye, M. Martinez, and M. Monperrus, “Neural program repair\\nwith execution-based backpropagation,” in ICSE , 2022.\\n[55] Q. Zhang, C. Fang, Y . Ma, W. Sun, and Z. Chen, “A survey of\\nlearning-based automated program repair,” ACM Transactions on', metadata={'source': 'test.pdf', 'page': 11}),\n",
       " Document(page_content='with execution-based backpropagation,” in ICSE , 2022.\\n[55] Q. Zhang, C. Fang, Y . Ma, W. Sun, and Z. Chen, “A survey of\\nlearning-based automated program repair,” ACM Transactions on\\nSoftware Engineering and Methodology , vol. 33, no. 2, pp. 1–69,\\n2023.\\n[56] H. Joshi, J. P. C. S ´anchez, S. Gulwani, V . Le, G. Verbruggen,\\nand I. Radicek, “Repair is nearly generation: Multilingual\\nprogram repair with llms,” in Thirty-Seventh AAAI Conference\\non Artificial Intelligence, AAAI 2023, Washington, DC, USA,\\nFebruary 7-14, 2023 , B. Williams, Y . Chen, and J. Neville,\\nEds. AAAI Press, 2023, pp. 5131–5140. [Online]. Available:\\nhttps://doi.org/10.1609/aaai.v37i4.25642\\n[57] D. Shrivastava, H. Larochelle, and D. Tarlow, “Repository-\\nlevel prompt generation for large language models of code,” in\\nInternational Conference on Machine Learning . PMLR, 2023,\\npp. 31 693–31 715.\\n[58] C. S. Xia, M. Paltenghi, J. L. Tian, M. Pradel, and L. Zhang,\\n“Fuzz4all: Universal fuzzing with large language models,” in', metadata={'source': 'test.pdf', 'page': 11}),\n",
       " Document(page_content='pp. 31 693–31 715.\\n[58] C. S. Xia, M. Paltenghi, J. L. Tian, M. Pradel, and L. Zhang,\\n“Fuzz4all: Universal fuzzing with large language models,” in\\nICSE , 2024.\\n[59] C. Lemieux, J. P. Inala, S. K. Lahiri, and S. Sen, “Codamosa:\\nEscaping coverage plateaus in test generation with pre-trained\\nlarge language models,” in 45th International Conference on\\nSoftware Engineering, ser. ICSE , 2023.\\n[60] M. Sch ¨afer, S. Nadi, A. Eghbali, and F. Tip, “An empirical\\nevaluation of using large language models for automated unit\\ntest generation,” IEEE Trans. Software Eng. , vol. 50, no. 1, pp.\\n85–105, 2024. [Online]. Available: https://doi.org/10.1109/TSE.\\n2023.3334955\\n[61] G. Ryan, S. Jain, M. Shang, S. Wang, X. Ma, M. K. Ramanathan,\\nand B. Ray, “Code-aware prompting: A study of coverage guided\\ntest generation in regression setting using llm,” in FSE, 2024.\\n[62] N. Alshahwan, J. Chheda, A. Finegenova, B. Gokkaya,\\nM. Harman, I. Harper, A. Marginean, S. Sengupta, and E. Wang,', metadata={'source': 'test.pdf', 'page': 11}),\n",
       " Document(page_content='test generation in regression setting using llm,” in FSE, 2024.\\n[62] N. Alshahwan, J. Chheda, A. Finegenova, B. Gokkaya,\\nM. Harman, I. Harper, A. Marginean, S. Sengupta, and E. Wang,\\n“Automated unit test improvement using large language models\\nat meta,” in FSE, vol. abs/2402.09171, 2024. [Online]. Available:\\nhttps://doi.org/10.48550/arXiv.2402.09171\\n[63] S. Kang, J. Yoon, and S. Yoo, “Large language models are\\nfew-shot testers: Exploring llm-based general bug reproduction,”\\nin45th IEEE/ACM International Conference on Software\\nEngineering, ICSE , 2023, pp. 2312–2323. [Online]. Available:\\nhttps://doi.org/10.1109/ICSE48619.2023.00194', metadata={'source': 'test.pdf', 'page': 11}),\n",
       " Document(page_content='[64] S. Feng and C. Chen, “Prompting is all your need: Automated\\nandroid bug replay with large language models,” in ICSE , 2024.\\n[65] R. Bairi, A. Sonwane, A. Kanade, V . D. C, A. Iyer,\\nS. Parthasarathy, S. Rajamani, B. Ashok, and S. Shet, “Codeplan:\\nRepository-level coding using llms and planning,” 2023.\\n[66] L. Gao, A. Madaan, S. Zhou, U. Alon, P. Liu, Y . Yang,\\nJ. Callan, and G. Neubig, “PAL: program-aided language\\nmodels,” CoRR , vol. abs/2211.10435, 2022. [Online]. Available:\\nhttps://doi.org/10.48550/arXiv.2211.10435', metadata={'source': 'test.pdf', 'page': 12})]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Vector Embedding And Vector Store\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "db = Chroma.from_documents(documents,OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fig. 10: Frequency of tool invocations (average per bug).\n",
      "the Java projects during training. While we acknowl-\n",
      "edge this risk, our approach does not solely depend on\n",
      "knowing about a bug, but rather the ability to collect\n",
      "information to fix the bug. We also note that the closest\n",
      "competitor, ChatRepair, also uses GPT-3.5, and thus\n",
      "faces the same risk. (ii) Missing test cases: Defects4J\n",
      "has at least one failing test case for each bug, which\n",
      "may not be the case for real-world usage scenarios. It\n",
      "will be interesting to evaluate RepairAgent on bugs with\n",
      "no a-priori available error-revealing test cases in future\n",
      "work. (iii) Fault localization: Inaccurate or imprecise\n",
      "fault localization could lead to suboptimal repair sug-\n",
      "gestions or incorrect diagnoses. (iv) Non-deterministic\n",
      "output of LLMs: The inherently non-deterministic nature\n",
      "of LLMs may result in different outcomes between two\n",
      "consecutive runs of RepairAgent. The large number of\n",
      "bugs we evaluate on mitigates this risk. Moreover, we\n"
     ]
    }
   ],
   "source": [
    "query = \"Better results\"\n",
    "retireved_results=db.similarity_search(query)\n",
    "print(retireved_results[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FAISS Vector Database\n",
    "from langchain_community.vectorstores import FAISS\n",
    "db = FAISS.from_documents(documents[:15], OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the LLM and the tools. We present novel techniques for\n",
      "guiding tool invocations through a finite state machine\n",
      "and for heuristically interpreting possibly incorrect LLM\n",
      "outputs. The iterative loop of RepairAgent continues\n",
      "until the agent declares to have found a suitable fix, or\n",
      "until exhausting a budget of iterations.\n",
      "To evaluate the effectiveness of our approach, we\n",
      "apply it to all 835 bugs in the Defects4J [24] dataset,\n",
      "a widely used benchmark for evaluating program repair\n",
      "techniques. RepairAgent successfully fixes 164 bugs,\n",
      "including 74 and 90 bugs of Defects4J v1.2 and v2.0,\n",
      "respectively. The correctly fixed bugs include 49 bugs\n",
      "that require fixing more than one line, showing that\n",
      "RepairAgent is capable of fixing complex bugs. Com-\n",
      "pared to state-of-the-art techniques [19], [21], Repair-\n",
      "Agent successfully fixes 39 bugs not fixed by prior work.\n",
      "Measuring the costs imposed by interacting with the\n",
      "LLM, we find that RepairAgent imposes an average cost\n"
     ]
    }
   ],
   "source": [
    "query = \"Better results\"\n",
    "retireved_results=db.similarity_search(query)\n",
    "print(retireved_results[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
